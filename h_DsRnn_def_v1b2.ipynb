{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h_DsRnn_def.ipynb\n",
    "\n",
    "## Imports for h_DsRnn_v*.ipynb\n",
    "\n",
    "SEE ALSO: `h_DsRnn_def_v1b.ipynb`\n",
    "\n",
    "PREC: `h_DsRnn_v3b8.ipynb`, `h_DsRnn_def_v1b.ipynb`\n",
    "\n",
    "AUTH: Hendrik.Mandelkow@nih.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n",
    "#### Biowulf preliminaries:\n",
    "```bash\n",
    "module load python/3.6\n",
    "...OR\n",
    "conda_on\n",
    "conda activate Tf1\n",
    "```\n",
    "\n",
    "##### NB: GPU is required for TF backend!\n",
    "```bash\n",
    "freengpu # free nodes\n",
    "sinteractive -TT -t 24:00:00 -c 14 --mem=16g --gres=gpu:p100:1,lscratch:32'\n",
    "alias sintg='f(){ sinteractive -TT -t 36:00:00 -c $((14*${1##*:})) --mem=64g --gres=gpu:$1,lscratch:10; }; f'\n",
    "sintg 1\n",
    "sintg p100:1\n",
    "\n",
    "sjobs\n",
    "newwall --jobid ??? --time 36:00:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./TB_logs --port $PORT2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Guess this must come before imports!?!\n",
    "# HOWTO enable autoreload for imported modules.\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# %matplotlib auto\n",
    "%matplotlib inline\n",
    "#%aimport ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import sys, os, re, glob\n",
    "import numpy as np\n",
    "np.set_printoptions(3)\n",
    "%precision 3\n",
    "# import scipy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.style.use('dark_background')\n",
    "from pprint import pprint\n",
    "import scipy\n",
    "from scipy import signal as scsi\n",
    "import hdf5storage as hdf5\n",
    "import nibabel as niba\n",
    "import nilearn as nile\n",
    "import nilearn.plotting as niplt\n",
    "# HOWTO suppress warnings:\n",
    "from warnings import warn, filterwarnings, simplefilter\n",
    "filterwarnings('ignore',module='nilearn',lineno=1569)\n",
    "# simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import htools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '/hpy:' not in ':'.join(sys.path)+':':\n",
    "    sys.path.insert(0,'/home/mandelkowhc/matlab/htools1/hpy')\n",
    "# pprint(sys.path[:5])\n",
    "os.chdir('/home/mandelkowhc/matlab/htools1/hpy')\n",
    "!jupyter nbconvert --to python --TemplateExporter.exclude_output=True --TemplateExporter.exclude_raw=True --TemplateExporter.exclude_markdown=False htools_v1b.ipynb\n",
    "\n",
    "from htools_v1b import hipymagic, hipyshell, hcd, hstd, hmovmean, hhline, hvline, hscalez, hreshape, \\\n",
    "    hFpath, hFname, htcode64, htime64, himgtileax, hxcorry, hnormalize, hrescale, hsavefig, ddict, hsys\n",
    "#    hFpath, hFname, Fbase, htcode64\n",
    "import hbiopack as hbp\n",
    "import hDsCl as hds\n",
    "# from h_BpRnnBw_def import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrms = lambda X,d: np.sqrt(np.mean(np.abs(X)**2,d))\n",
    "hrss = lambda X,d: np.sqrt(np.sum(np.abs(X)**2,d))\n",
    "hnorm2 = lambda X,d: np.sqrt(np.sum(np.abs(X)**2,d))\n",
    "hcorrxy = lambda x,y: np.corrcoef(x,y,rowvar=False)[:x.shape[1],x.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hplotstyles = lambda : [ plt.style.use('dark_background'), mpl.rcParams.update({'figure.figsize': (18,4)}) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hind2sub = np.unravel_index # lin.index to multi-subscripts\n",
    "hsub2ind = np.ravel_multi_index # to lin.index\n",
    "\n",
    "def hPutMaskSorted(M,X,x=0):\n",
    "    '''[***] Similar to np.putmask but with sorted M.values\n",
    "    '''\n",
    "    # np.put( np.zeros(M.shape, X.dtype), np.unravel_index(np.argsort(M,None)[-X.size:],M.shape), X )\n",
    "    # Img = np.zeros(list(M.shape)+X.shape[1:], X.dtype) + x\n",
    "    # Img = np.zeros(M.shape, X.dtype) + x\n",
    "    Img = np.full(M.shape, x, dtype=X.dtype)\n",
    "    # [+++] HOWTO assign vlues to an ordered mask:\n",
    "    Img[np.unravel_index(np.argsort(M,None)[-X.size:],M.shape)] = X\n",
    "    return Img\n",
    "\n",
    "# Use different name + input sequence?\n",
    "hma2im = lambda X,M,x=0: hPutMaskSorted(M,X,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script _bash\n",
    "cd $ExId.results\n",
    "set +e # don't exit on error\n",
    "rm -f McPar.1D\n",
    "ln -s dfile.r01.1D McPar.1D || echo Link exists.\n",
    "3dAFNItoNIFTI -overwrite -prefix Epi_mask.nii.gz full_mask.*.BRIK*\n",
    "if $OW || [ ! -e Epi_Mc.nii* ] ; then\n",
    "\t3dAFNItoNIFTI -overwrite -float -prefix Epi_Mc.nii.gz pb01.*.BRIK*\n",
    "fi\n",
    "if $OW || [ ! -e Epi_Mcr.nii* ] ; then\n",
    "\t3dAFNItoNIFTI -overwrite -float -prefix Epi_Mcr.nii.gz errts.*.BRIK*\n",
    "fi\n",
    "if $OW || [ ! -e Epi_Mcr_std.nii* ] ; then\n",
    "\t3dTstat -overwrite -stdev -prefix Epi_Mcr_std.nii.gz Epi_Mcr.nii*\n",
    "fi\n",
    "if $OW || [ ! -e Epi_Mc_mean.nii* ] ; then\n",
    "\t# 3dTstat -overwrite -mean -std -prefix Epi_Mc_mean+std.nii.gz Epi_Mc.nii*\n",
    "\t3dTstat -overwrite -mean -prefix Epi_Mc_mean.nii.gz Epi_Mc.nii*\n",
    "\t3dTstat -overwrite -stdev -prefix Epi_Mc_std.nii.gz Epi_Mc.nii*\n",
    "fi\n",
    "# find . -iname \"*mask*.BRIK\" -print -exec 3dAFNItoNIFTI {} \\;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKVAL = +0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************\n",
    "# Batch generator\n",
    "Training a *stateful* RNN on multiple (independent) voxels in parallel requires specially formed batches of training data. These could either be created and fed manually using model.train_on_batch() or by using model.fit_generator() with a custom generator - see below.\n",
    "\n",
    "#### Re: Seq. length\n",
    "In simple Keras the seq. length is the same for training and prediction. It determines e.g. the \"depth\" of backprop in time. If the RNN *stateful* stride should equal seq. length and the batch size Nbatch equal to 1 or, perhaps, the number of inputs (voxels) trained in parallel. If the network is *not* stateful prediction may require longer sequences, but they can overlap i.e. the *stride* can be short e.g. 1TR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def hXtv2Data( X, Y, dt, t0=None, MaskVal=MASKVAL, MaskCol=False, Sparse=False):\n",
    "    '''[***3ab] Cat time series of unequal sampling rate.\n",
    "    Cat X with Y upsampled by dt with t0 offset and MaskVal between samples.\n",
    "    \n",
    "    USE: Data[:,:NX+NY] = hXtv2Data( Bp.Fata[:,:NX], Xtv[:,:NY], Bp.Fs*TR )\n",
    "    \n",
    "    RETURNS: Data with Data[:,:NX] = X[:,NX] and Data[ t0::dt, NX:] = Y[:,:NY]\n",
    "    \n",
    "    MaskVal: (scalar float) used to interpolate Y\n",
    "    MaskCol: if True prepend Y with a \"boolean\" input mask column like:\n",
    "        Y = np.c_[ Y[:,0]*0+1, Y ] and NX += 1\n",
    "    Sparse: if True return (mem.efficient) sparse type csr_matrix\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if MaskCol: # Prepend a mask column of 0/1\n",
    "        Y = np.c_[ Y[:,0]*0+1, Y ]\n",
    "        # Y = np.c_[ np.ones_like(Y[:,0]), Y ]\n",
    "    '''\n",
    "    \n",
    "    NX,NY = X.shape[1],Y.shape[1]\n",
    "    if t0 is None:\n",
    "        t0 = dt-1\n",
    "\n",
    "    if Sparse:\n",
    "        # CSR: Sparse matrix stored in contiguous rows:\n",
    "        tmp = min( Y.shape[0], X.shape[0]//dt)\n",
    "        Data = csr_matrix( (tmp*dt+t0, NX+NY), np.float32)\n",
    "    else:\n",
    "        tmp = min( Y.shape[0], X.shape[0]//dt)\n",
    "        Data = np.zeros( (tmp*dt, NX+NY), np.float32)\n",
    "\n",
    "    if MaskVal:\n",
    "        Data[...] = MaskVal # Use masking value for Xtv\n",
    "        \n",
    "    print(X.shape,end=' '); print(Y.shape,end=' '); print(Data.shape)\n",
    "    Data[:,:NX] = X[:Data.shape[0],:] # +++\n",
    "    # Data[t0:Y.shape[0]*dt:dt, NX:] = Y[:,:NY]\n",
    "    Data[t0::dt, NX:] = Y[:,:NY]\n",
    "    \n",
    "    if MaskCol:\n",
    "        # numpy.insert(arr, obj, values, axis=None)\n",
    "        Data = np.insert( Data, NX, Data[:,-1]!=MaskVal, axis=1)\n",
    "    \n",
    "    return Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v4b new ValFrac\n",
    "See h_DsRnn_def_v1b.ipynb for older / alternate versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    " * [x] add dropout for X(MRI)\n",
    " * [ ] add Tsh for augmentation\n",
    " * possibly avoid cp of data by reshaping batch instead of data?\n",
    " * Incorporate upsampling of MRI data?\n",
    " * set shift = 0 to test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hBatchSeq1y(keras.utils.Sequence):\n",
    "    '''[**4b++] Batch generator (keras.*.Sequence) for stateful RNN with NX,NY,NB > 1.\n",
    "    Stack NB and NY into a batch \n",
    "    For use with STATEFUL=True\n",
    "    Returns: ( X[NY*NB,NT,NX+1], Y[NY*NB,NT,1]) or ( X, Y, Mask )\n",
    "    \n",
    "    Data[t,:NX+NY]\n",
    "    NT: length of each training sequence [ batch_size=(NY*NB, NT, NX+1) ]\n",
    "    NB: split Data[t,c] into NB sections along t for parallel training\n",
    "    NY= Data.shape[-1]-NX : split Data[t,:NX+NY] for parallel training of each NY\n",
    "    Drop: Dropout 0 < Drop < 1.0, mask Drop*100% of input Y at random [DropMode= 'sample']\n",
    "        DropMode='sequence' # drop Y input for entire samples (sequences - NT) at random\n",
    "        DropMode='odd' # drop Y input for odd samples (sequences - NT)\n",
    "        DropMode='last' # drop Y input for last samples (sequences - NT) in each section\n",
    "    Mask = sample_weights = either 1D array of Batch.shape[0] or 2D of Batch.shape[:2]\n",
    "        Return ( X, Y, sample_weights) to serve as a mask for cost functions\n",
    "        Need to set sample_weight_mode = 'temporal' ?!?\n",
    "    ValFrac: if >0 leave out ValFrac*100% at the end of each section\n",
    "    WARNING: This results in *distributed* val.data e.g. NB=4, ValFrac=1/3 -> TTV,TTV,TTV,TTV\n",
    "    ValFrac > 0 : deliver training data\n",
    "    ValFrac < 0 : deliver validation data\n",
    "    \n",
    "    .getX() : retrieve X (full TS)\n",
    "    .getY() : retrieve Y for comparison with Yh\n",
    "    .predict( model ) : compute Yh\n",
    "    .evaluate( model ) : compute losses\n",
    "    .reshapeInput : change model input_shape for prediction on different batch size\n",
    "    ...\n",
    "    \n",
    "    SEE: h_BpRnnBw_def.py\n",
    "    '''\n",
    "    # AUTH: Hendrik.Mandelkow@gmail.com\n",
    "    \n",
    "    def __init__(self, Data, NT, NX=None, NB=1, ValFrac=0, Mask=None, Drop=0, DropMode='sample'):\n",
    "        # self.__dict__.update(Data=Data, NT=NT, NY=NY, NB=NB, NX=Data.shape[-1]-NY)\n",
    "        self.__dict__.update(Data=Data, NT=NT, NX=NX, NB=NB, Mask=Mask, Drop=Drop, DropMode=DropMode, MaskVal=-10)\n",
    "        try: self.Drop, self.DropMode = self.Drop[0], self.Drop[1]\n",
    "        except: pass\n",
    "        self.MaskVal = +0.0; warn('+++ TEST +++ MaskVal.')\n",
    "        self.NX = Data.shape[-1]-1 if NX is None else NX\n",
    "        self.NY = Data.shape[-1] - self.NX\n",
    "        \n",
    "        # self.Data[:,self.NX:] = hzscore(self.Data[:,self.NX:])\n",
    "\n",
    "        self.Data = Data[:Data.shape[0]//NT//NB*NB*NT,:].reshape(NB,-1,NT,Data.shape[-1]) # [NB,B,NT,NX+NY]\n",
    "        self.Data = np.moveaxis(self.Data,0,1) # [B,NB,NT,NX+NY]\n",
    "        if ValFrac:\n",
    "            print('+++ WARNING: *Interleaved* validation data at the end of each block.')\n",
    "        if ValFrac > 0:\n",
    "            print('+ Training data.')\n",
    "            self.Data = self.Data[:-round(abs(ValFrac)*self.Data.shape[0])]\n",
    "        elif ValFrac < 0:\n",
    "            print('+ Validation data.')\n",
    "            self.Data = self.Data[-round(abs(ValFrac)*self.Data.shape[0]):]\n",
    "\n",
    "        print('Batches per epoch: %u, batch size (NY*NB): %u' \\\n",
    "              %( self.Data.shape[0], self.Data.shape[1]*self.NY))\n",
    "        \n",
    "        assert self.Data.size > 0, 'Oops, Data.shape= '+str(Data.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Data.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx): # return one *batch*!\n",
    "        NT,NX,NY,NB = map(self.__dict__.get, ['NT','NX','NY','NB'])\n",
    "        X = self.Data[idx] # One batch: X[NB,NT,NX+NY]\n",
    "        if True:\n",
    "            X = np.concatenate([ X[:,:,[*range(NX),NX+y]] for y in range(NY)], axis=0) # X[NY*NB,NT,NX+1]\n",
    "        else:\n",
    "            assert False,'NOT TESTED!'\n",
    "            warn('NOT TESTED!')\n",
    "            X = np.concatenate((np.tile(X[:,:,:NX],[1,1,1,NY]),X[:,:,None,:]),2) # [NB,NT,NX+1,NY]\n",
    "            X = np.moveaxis(X,-1,0).reshape(NY*NB,NT,NX+1) # X[NY*NB,NT,NX+1]\n",
    "\n",
    "        ### Shift Y in time to make prediction non-trivial.\n",
    "        # Circshift each seq. (NT) may be suboptimal but simple and irrelevant.\n",
    "        Y = np.roll(X[:,:,-1:],-1,axis=-2) # +++ Y[NY*NB,NT,1] (out) shifted -1 rel to X\n",
    "        # Y = np.roll(X[:,:,-1:],0,axis=1); warn('TEST TEST TEST!')\n",
    "        \n",
    "        ### Dropout to decrease reliance on BOLD autocorrelations\n",
    "        if self.Drop:\n",
    "            assert (0 <= self.Drop <= 1), 'Oops! Expecting 0 < Drop < 1.'\n",
    "            tmp = self.DropMode[:3].lower()\n",
    "            if tmp in ['sam']: # samples\n",
    "                X[ np.random.random(X.shape[:-1])<self.Drop, -1] = self.MaskVal # ***\n",
    "            if tmp in ['seq']: # sequences\n",
    "                X[ np.random.random(X.shape[:1])<self.Drop, :, -1] = self.MaskVal # ***\n",
    "            if tmp in ['odd']: # odd sequences 1,3,5,...\n",
    "                if (idx % 2): X[ :, :, -1] = self.MaskVal # ***\n",
    "            if tmp in ['las']: # drop last len()*Drop sequences \n",
    "                if idx/(len(self)-1)>(1-self.Drop): X[ :, :, -1] = self.MaskVal # ***\n",
    "            if np.all( np.logical_or( X[...,-2]==0, X[...,-2]==1) ):\n",
    "                X[...,-2] = X[...,-1]!=self.MaskVal\n",
    "        \n",
    "        ### Return mask?\n",
    "        #< X[X==np.nan] = self.MaskVal\n",
    "        #< X[ np.isnan(X[...,-1]), -1] = self.MaskVal\n",
    "        #< assert not np.any(np.isnan(X)), 'Oops NaNs in X!?!'\n",
    "        if self.Mask is None:\n",
    "            return ( X, Y )\n",
    "        elif isinstance( self.Mask, np.ndarray ):\n",
    "            # assert False, 'Not tested!?!'\n",
    "            return ( X, Y, self.Mask )\n",
    "            # Could use Mask = np.any(Y,-1).astype(float)\n",
    "        else:\n",
    "            assert False, 'TEST! Not this way!'\n",
    "            Mask = np.any( Y != self.Mask, -1).astype(float) # Mask[NY*NB,NT] might be correct?!?\n",
    "            #< Mask = np.any( np.logical_not(np.isnan(Y)), -1).astype(float) # Mask[NY*NB,NT] might be correct?!?\n",
    "            return ( X, Y, Mask ) # (..., sample_weights)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            assert False, 'Not tested!?!'\n",
    "            assert self.Mask.size > 1, 'Not an array!?'\n",
    "            return ( X, Y, self.Mask )\n",
    "            # Could use Mask = np.any(Y,-1).astype(float)\n",
    "        except (AttributeError): # AssertionError\n",
    "            # assert False, 'Error: Work in progress.'\n",
    "            Mask = np.any( Y != self.Mask, 2).astype(float) # Mask[NY*NB,NT] might be correct?!?\n",
    "            return ( X, Y, Mask )\n",
    "        \"\"\"\n",
    "    \n",
    "    def getX(Bgen):\n",
    "        '''[*1a+]\n",
    "        '''\n",
    "        # PREC: hBatchGen_getX()\n",
    "        # OK this works, according to the test below.\n",
    "        XY = 0 # XY = 0,1 = getX, getY\n",
    "        NT,NX,NY,NB = map(Bgen.__dict__.get, ['NT','NX','NY','NB'])\n",
    "        x = np.stack([ Bgen[n][0] for n in range(len(Bgen)) ],1) # y[NY*NB,B,NT,NX+1]\n",
    "        assert NT == x.shape[-2], 'Oops!'\n",
    "        assert NB == x.shape[0]//NY, 'Oops!'\n",
    "        assert (NX+1) == x.shape[-1], 'Oops!'\n",
    "        x = x.reshape(NY,NB,len(Bgen),NT,NX+1) # [NY,NB,B,NT,NX+1]\n",
    "        x = np.concatenate((x[0,:,:,:,:NX], np.moveaxis(x[:,:,:,:,-1],0,-1)),-1)\n",
    "        x = x.reshape(-1,x.shape[-1])\n",
    "        return x\n",
    "\n",
    "    def getY(Bgen,Tsh=False):\n",
    "        '''[*1a+]\n",
    "        Tsh=True : Undo t-shift for training.\n",
    "        Tsh=False : directly comparable to Yh\n",
    "        NB : batch size, nof samples per batch\n",
    "        B : nof batches per epoch\n",
    "        '''\n",
    "        XY = 1 # get Y\n",
    "        NT,NX,NY,NB = map(Bgen.__dict__.get, ['NT','NX','NY','NB'])\n",
    "        y = np.stack([ Bgen[n][XY] for n in range(len(Bgen)) ],1) # y[NY*NB,B,NT,1]\n",
    "        assert NT == y.shape[-2], 'Oops!'\n",
    "        assert NB == y.shape[0]//NY, 'Oops!'\n",
    "        y = np.reshape(y,(NY,NB,len(Bgen),NT)) # [NY,NB,B,NT]\n",
    "        # y = np.transpose(y,(1,2,3,0)) # y[NB,B,NT,NY]\n",
    "        y = np.moveaxis(y,0,-1) # y[NB,B,NT,NY]\n",
    "        if Tsh:\n",
    "            y = np.roll(y,1,-2)\n",
    "        y = y.reshape(-1,y.shape[-1]) # y[NB*N*NT,NY]\n",
    "        return y\n",
    "\n",
    "    def unbatchYh(self,Yh,NY=None,NB=None,Tsh=False):\n",
    "        # This could be a static function Yh2Y()?\n",
    "        '''\n",
    "        # Yh[B*NY*NB,NT,1]\n",
    "        Yh[t,NY] = hUnbatchYh( RNN.predict_generator( hBatchSeq1y(...)))\n",
    "        '''\n",
    "        NY = self.NY if NY is None else NY\n",
    "        NB = self.NB if NB is None else NB\n",
    "        NT = Yh.shape[-2]\n",
    "        yh = np.reshape(Yh,(-1,NY,NB,NT)) # yh[B,NY,NB,NT]\n",
    "        yh = np.transpose(yh,(2,0,3,1)) #yh[NB,B,NT,NY]\n",
    "        if Tsh:\n",
    "            yh = np.roll(yh,1,-2)\n",
    "        yh = yh.reshape(-1,yh.shape[-1]) # yh[NB*B*NT,NY]\n",
    "        return yh\n",
    "    \n",
    "    def predict(Bgen, RNN, Tsh=False, Reset=True):\n",
    "        '''Run generator batches through RNN and reshuffle output into an array.\n",
    "        Bgen = TrainGen or ValidGen\n",
    "        '''\n",
    "        if Reset:\n",
    "            RNN.reset_states()\n",
    "        Yh = RNN.predict_generator(Bgen,verbose=1) # Yh[B*NY*NB,NT,1]\n",
    "        #< print(Yh.shape)\n",
    "        # Yh = hUnbatchYh(Yh,Bgen.NY,Bgen.NB,Tsh)\n",
    "        Yh = Bgen.unbatchYh(Yh,Bgen.NY,Bgen.NB,Tsh)\n",
    "        return Yh\n",
    "\n",
    "    def predict1(Bgen, RNN, Tsh=False, Reset=True):\n",
    "        '''Predict using new RNN with input shape matching Bgen.\n",
    "        E.g. use NB=1 for better stateful prediction.\n",
    "        Bgen = TrainGen or ValidGen\n",
    "        '''\n",
    "        # RNNp = keras.models.clone_model(RNN)\n",
    "        RNNp = keras.models.model_from_json(RNN.to_json())\n",
    "        # RNNp._layers[1].batch_input_shape = (NY,NT,NX+1)\n",
    "        RNNp._layers[1].batch_input_shape = Bgen[0][0].shape\n",
    "        RNNp = keras.models.model_from_json(RNNp.to_json())\n",
    "        RNNp.set_weights(RNN.get_weights())\n",
    "        # [ RNNp.layers[n].set_weights(RNN.layers[n].get_weights()) for n in range(len(RNN.layers))]\n",
    "        # ??? [ L.stateful= True for L in RNNp.layers if hasattr(L,'stateful') ]\n",
    "        # ??? [ L.batch_input_shape= (1,None,3) for L in RNNp.layers if hasattr(L,'stateful') ]\n",
    "        RNNp.summary()\n",
    "\n",
    "        if Reset:\n",
    "            RNNp.reset_states()\n",
    "        Yh = RNNp.predict_generator(Bgen,verbose=1) # Yh[B*NY*NB,NT,1]\n",
    "        #< print(Yh.shape)\n",
    "        # Yh = hUnbatchYh(Yh,Bgen.NY,Bgen.NB,Tsh)\n",
    "        Yh = Bgen.unbatchYh(Yh,Bgen.NY,Bgen.NB,Tsh)\n",
    "        return Yh\n",
    "\n",
    "    def reshapeInput(Bgen, RNN, InputShape=None):\n",
    "        '''Cp RNN weights to new model with batch_input_shape matching generator (for prediction)\n",
    "        Bgen = TrainGen or ValidGen\n",
    "        RNN\n",
    "        InputShape = batch_input_shape = (NB,NT,NY)\n",
    "        '''\n",
    "        # RNNp = keras.models.clone_model(RNN)\n",
    "        RNNp = keras.models.model_from_json(RNN.to_json())\n",
    "        # RNNp._layers[1].batch_input_shape = (NY,NT,NX+1)\n",
    "        if InputShape in None: InputShape =  Bgen[0][0].shape\n",
    "        # RNNp._layers[1].batch_input_shape = Bgen[0][0].shape\n",
    "        RNNp._layers[1].batch_input_shape = InputShape\n",
    "        RNNp = keras.models.model_from_json(RNNp.to_json())\n",
    "        RNNp.set_weights(RNN.get_weights())\n",
    "        # [ RNNp.layers[n].set_weights(RNN.layers[n].get_weights()) for n in range(len(RNN.layers))]\n",
    "        # ??? [ L.stateful= True for L in RNNp.layers if hasattr(L,'stateful') ]\n",
    "        # ??? [ L.batch_input_shape= (1,None,3) for L in RNNp.layers if hasattr(L,'stateful') ]\n",
    "        RNNp.summary()\n",
    "\n",
    "        return RNNp\n",
    "\n",
    "    def evaluate(Bgen, RNN, Reset=True):\n",
    "        '''\n",
    "        Bgen = TrainGen or ValidGen\n",
    "        '''\n",
    "        if Reset:\n",
    "            RNN.reset_states()\n",
    "        Losses = RNN.evaluate_generator(Bgen,verbose=1) # Yh[B*NY*NB,NT,1]\n",
    "        Losses = dict( zip( RNN.metrics_names, Losses))\n",
    "        return Losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hUnbatchYh(Yh,NY,NB,Tsh=False):\n",
    "    '''\n",
    "    # Yh[B*NY*NB,NT,1]\n",
    "    Yh[t,NY] = hUnbatchYh( RNN.predict_generator( hBatchSeq1y(...)))\n",
    "    '''\n",
    "    NT = Yh.shape[-2]\n",
    "    yh = np.reshape(Yh,(-1,NY,NB,NT)) # yh[B,NY,NB,NT]\n",
    "    yh = np.transpose(yh,(2,0,3,1)) #yh[NB,B,NT,NY]\n",
    "    if Tsh:\n",
    "        yh = np.roll(yh,1,-2)\n",
    "    yh = yh.reshape(-1,yh.shape[-1]) # yh[NB*B*NT,NY]\n",
    "    return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hPredictGen(RNN, Bgen, Tsh=False, Reset=True):\n",
    "    '''\n",
    "    Bgen = TrainGen or ValidGen\n",
    "    '''\n",
    "    if Reset:\n",
    "        RNN.reset_states()\n",
    "    Yh = RNN.predict_generator(Bgen,verbose=1) # Yh[B*NY*NB,NT,1]\n",
    "    print(Yh.shape)\n",
    "    Yh = hUnbatchYh(Yh,Bgen.NY,Bgen.NB,Tsh)\n",
    "    return Yh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hBatchGen_getY(Bgen,Tsh=False):\n",
    "    '''[*1a+]\n",
    "    Tsh=True : Undo t-shift for training.\n",
    "    Tsh=False : directly comparable to Yh\n",
    "    NB : batch size, nof samples per batch\n",
    "    B : nof batches per epoch\n",
    "    '''\n",
    "    XY = 1 # get Y\n",
    "    NT = Bgen.NT\n",
    "    NY = Bgen.NY\n",
    "    NB = Bgen.NB\n",
    "    y = np.stack([ Bgen[n][XY] for n in range(len(Bgen)) ],1) # y[NY*NB,B,NT,1]\n",
    "    assert NT == y.shape[-2], 'Oops!'\n",
    "    assert NB == y.shape[0]//NY, 'Oops!'\n",
    "    y = np.reshape(y,(NY,NB,len(Bgen),NT)) # [NY,NB,B,NT]\n",
    "    # y = np.transpose(y,(1,2,3,0)) # y[NB,B,NT,NY]\n",
    "    y = np.moveaxis(y,0,-1) # y[NB,B,NT,NY]\n",
    "    if Tsh:\n",
    "        y = np.roll(y,1,-2)\n",
    "    y = y.reshape(-1,y.shape[-1]) # y[NB*N*NT,NY]\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hBatchGen_getX(Bgen):\n",
    "    '''[*1a+]\n",
    "    '''\n",
    "    # OK this works, according to the test below.\n",
    "    XY = 0 # XY = 0,1 = getX, getY\n",
    "    NT = Bgen.NT\n",
    "    NX = Bgen.NX\n",
    "    NY = Bgen.NY\n",
    "    NB = Bgen.NB\n",
    "    x = np.stack([ Bgen[n][0] for n in range(len(Bgen)) ],1) # y[NY*NB,B,NT,NX+1]\n",
    "    assert NT == x.shape[-2], 'Oops!'\n",
    "    assert NB == x.shape[0]//NY, 'Oops!'\n",
    "    assert (NX+1) == x.shape[-1], 'Oops!'\n",
    "    x = x.reshape(NY,NB,len(Bgen),NT,NX+1) # [NY,NB,B,NT,NX+1]\n",
    "    x = np.concatenate((x[0,:,:,:,:NX], np.moveaxis(x[:,:,:,:,-1],0,-1)),-1)\n",
    "    x = x.reshape(-1,x.shape[-1])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function: masked (weighted) L2\n",
    "Missing training data can be handled either by passing sample_weights [with sample_weight_mode='temporal'] for masking the loss function in model.fit(). Alternatively, we may define a custom loss function (with integrated mask) as seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Custom objects must be passed to model.compile and also model_load()\n",
    "if 'KCustoms' not in locals(): KCustoms = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Looks like loss functions can return either a scalar or an array that will be summed over.\n",
    "# I think sample_weight should require the array, but there is no error?! \n",
    "# Maybe some erroneous broadcast goint on?!?\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "import keras.backend as K\n",
    "def hMSE(Y, Yh):\n",
    "    if not K.is_tensor(Yh): Yh = K.constant(Yh)\n",
    "    Y = K.cast(Y, Yh.dtype)\n",
    "    return K.mean(K.square(Yh - Y),-1) # axis=-1 didn't matter?!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK\n",
    "import keras.backend as K\n",
    "def hWMSE(Y,Yh):\n",
    "    '''Weighted Mean Square Error (MSE)\n",
    "    '''\n",
    "    Mask= -10.0\n",
    "    Mask= +0.0 # +++ TEST +++\n",
    "    if not K.is_tensor(Yh): Yh = K.constant(Yh)\n",
    "    Y = K.cast(Y, Yh.dtype)\n",
    "    mask = K.not_equal(Y,Mask) # OK! could use NaN <-> 0\n",
    "    # TODO: assert there are masked values?!\n",
    "    # return K.mean(K.square(Yh[mask]-Y[mask])) # BUT: No boolean indexing in Keras!\n",
    "    mask = K.cast(mask,K.dtype(Y)) # OK!\n",
    "    return K.sum(K.square(K.abs(Yh - Y)*mask))/K.sum(mask) # OK!\n",
    "\n",
    "KCustoms['hWMSE'] = hWMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def hMSEmask(Mask=-10.0):\n",
    "    def LossFun_(Y,Yh):\n",
    "        '''Weighted Mean Square Error (MSE)\n",
    "        '''\n",
    "        if not K.is_tensor(Yh): Yh = K.constant(Yh)\n",
    "        Y = K.cast(Y, Yh.dtype)\n",
    "        mask = K.not_equal(Y,Mask) # OK! could use NaN <-> 0\n",
    "        # return K.mean(K.square(Yh[mask]-Y[mask])) # BUT: No boolean indexing in Keras!\n",
    "        mask = K.cast(mask,K.dtype(Y)) # OK!\n",
    "        return K.sum(K.square(K.abs(Yh - Y)*mask))/K.sum(mask) # OK!\n",
    "    return LossFun_\n",
    "\n",
    "# KCustoms['hMSEmask'] = hMSEmask(-10.0)\n",
    "warn('+++ TEST +++ Using alternate MaskVal!')\n",
    "KCustoms['hMSEmask'] = hMSEmask(+0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "class hWMSEmaskCl:\n",
    "    def __init__(self, Mask=-10.0):\n",
    "        self.Mask = Mask\n",
    "        \n",
    "    def __call__(self,Y,Yh):\n",
    "        '''Weighted Mean Square Error (MSE)\n",
    "        '''\n",
    "        if not K.is_tensor(Yh): Yh = K.constant(Yh)\n",
    "        Y = K.cast(Y, Yh.dtype)\n",
    "        mask = K.not_equal(Y,self.Mask) # OK! could use NaN <-> 0\n",
    "        # return K.mean(K.square(Yh[mask]-Y[mask])) # BUT: No boolean indexing in Keras!\n",
    "        mask = K.cast(mask,K.dtype(Y)) # OK!\n",
    "        return K.sum(K.square(K.abs(Yh - Y)*mask))/K.sum(mask) # OK!\n",
    "\n",
    "# KCustoms['hWMSE'] = hWMSEmaskCl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# NOTE: Y,Yh[NB,NT,NY] one batch!\n",
    "def hWRVF(Y,Yh):\n",
    "    '''Weighted Residual Variance Fraction\n",
    "    '''\n",
    "    Mask= -10.0\n",
    "    Mask= +0.0 # +++ TEST +++\n",
    "    if not K.is_tensor(Yh): Yh = K.constant(Yh) # need this??\n",
    "    Y = K.cast(Y, Yh.dtype) # need this??\n",
    "    mask = K.not_equal(Y,Mask) # OK! could use NaN <-> 0\n",
    "    # return K.sum(K.square(Yh[mask]-Y[mask])) # BUT: No boolean indexing in Keras!\n",
    "    mask = K.cast(mask,K.dtype(Y)) # OK!\n",
    "    return K.sum(K.square((Yh - Y)*mask)) / K.sum(K.square(Y*mask))\n",
    "\n",
    "KCustoms['hWRVF'] = hWRVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def hRVF(Y,Yh):\n",
    "    '''Residual Variance Fraction\n",
    "    return K.sum(K.square(Yh - Y)) / K.sum(K.square(Y))\n",
    "    '''\n",
    "    if not K.is_tensor(Yh): Yh = K.constant(Yh)\n",
    "    Y = K.cast(Y, Yh.dtype)\n",
    "    return K.sum(K.square(Yh - Y)) / K.sum(K.square(Y))\n",
    "\n",
    "KCustoms['hRVF'] = hRVF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider activation functions?!\n",
    "# AUTH: HM, 2019-06-26, v3b2: Add GRU, better handling of input layer.\n",
    "# AUTH: HM, 2019-06-26, v3b3: Add Nio=Type\n",
    "def mkRNN(Nio=[1,1], Nsteps=1, Nbatch=None, **kwarg):\n",
    "    '''[++-] Make simple stateful RNN with final dense layer.\n",
    "    Nsteps: number of time steps in each sample sequence\n",
    "    Nbatch: training batch size (number of sample sequences per batch)\n",
    "    TODO: What about initialization / regularization?\n",
    "    '''\n",
    "    PARS = {} # kwarg for Keras.layer...\n",
    "    # batch_input_shape = (batchsize,timesteps,data_dim) only for input layer\n",
    "    PARS['batch_input_shape'] = (Nbatch, Nsteps, Nio[0])\n",
    "    PARS['return_sequences']=True\n",
    "    PARS['stateful']=True # ***\n",
    "    # PARS['activation'] = 'linear' # BAD!\n",
    "    PARS.update(kwarg)\n",
    "    \n",
    "    RNN = keras.models.Sequential() # +++\n",
    "    # RNN.name = '' # clear generic name sequential_1\n",
    "    # The latest Keras (2.2.4?) does not seem to like name=''.\n",
    "    # Also, the name parameter seems to have some obscure internal uses. Better not touch.\n",
    "    \n",
    "    Type = 'L' # *** default\n",
    "    # RNN.name = str(Nio[0])+'L%u'*(len(Nio)-1)%tuple(Nio[1:])\n",
    "    # RNN.name = '%uL%u'%tuple(Nio[:2])\n",
    "    for Nout in Nio[1:]:\n",
    "        if isinstance(Nout,str):\n",
    "            Type = Nout\n",
    "            continue\n",
    "        if Type == 'L':\n",
    "            RNN.add(keras.layers.LSTM( Nout, **PARS))\n",
    "        elif Type == 'G':\n",
    "            # https://keras.io/layers/recurrent/#GRU\n",
    "            # keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid',...\n",
    "            RNN.add(keras.layers.GRU( Nout, implementation=2, **PARS))\n",
    "        elif Type == 'D': # Dense\n",
    "            RNN.add(keras.layers.Dense(1, activation='linear'))\n",
    "        else:\n",
    "            raise ValueError('Type must be L,G,D not: '+Type)\n",
    "            \n",
    "        #< RNN.name += Type+'%u'%Nout\n",
    "            \n",
    "    # tmp = [str([x.units for n,x in enumerate(y.layers)]) for y in RNNs]+['line %u'%x for x in range(n+2,len(h)+1)]\n",
    "    PARS.pop('batch_input_shape',None) # remove key, only required for input layer\n",
    "\n",
    "    # TODO: Use only hWMSE\n",
    "    # TODO: Could use MaskVal; hWRVFmask(MaskVal) here\n",
    "    # RNN.compile(loss=hWMSE, optimizer='adam', metrics=[hWRVF]) # +++\n",
    "    RNN.compile(loss=KCustoms['hMSEmask'], optimizer='adam', metrics=[ KCustoms['hWRVF'] ]) # +++\n",
    "    # RNN.compile(loss='MSE', optimizer='adam', metrics=[hWMSE,hWRVF], weighted_metrics=[hRVF,hMSE], sample_weight_mode='temporal') # TEST!!!\n",
    "    # NOTE: Apparently, sample_weights are applied to \"loss\" as well as weighted_metrics.\n",
    "    # TEST! RNN.compile(loss='MSE', optimizer='adam') # TEST!!!\n",
    "    print('+ RNN.name= '+RNN.name)\n",
    "    # Record input parameters:\n",
    "    tmp = locals() # Dunno why this is necessary?!\n",
    "    RNN.mkRNNargs = { n:tmp[n] for n in ['Nio','Nsteps','Nbatch'] }\n",
    "    RNN.mkRNNargs.update(kwarg)\n",
    "    return RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "def mkRnnGpu(Model,**par):\n",
    "    \"\"\"[+++] Recompile model for multi-GPU training: MModel = multi_gpu_model(Model, **par)\n",
    "    \n",
    "    PAR = { # optional parameters / defaults\n",
    "        'gpus':None, # Nof GPUs or None for all available\n",
    "        'cpu_merge':True # ?!? force merging of weights on CPU\n",
    "        'cpu_relocation':False, # ?!? force transfer of model from GPU to CPU\n",
    "        }\n",
    "        \n",
    "    WARNING: Use Model not MModel for saving:\n",
    "        model.save(Fname)\n",
    "        model.save_weights(Fname)\n",
    "        \n",
    "    MModel.fit() will split batches (evenly) across GPUs.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try: # Train on mult. GPUs\n",
    "        MModel = multi_gpu_model(Model, **par)\n",
    "    except: # Train on 1 CPU or GPU\n",
    "        MModel = Model\n",
    "\n",
    "    MModel.compile( loss= Model.loss, optimizer= Model.optimizer, metrics= Model.metrics )\n",
    "    \n",
    "    return MModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hResetStatesCb callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Why reset on epoch end?\n",
    "class hResetStatesCb(keras.callbacks.Callback):\n",
    "    '''\n",
    "    hResetStatesCb(False/True) # reset states at on_epoch_begin (default) / end\n",
    "    '''\n",
    "    def __init__(self,End=False): # required?\n",
    "        self.End = End\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if not self.End:\n",
    "            # HOWTO access model from callback:\n",
    "            self.model.reset_states()\n",
    "\n",
    "    # Doppeltgemoppelt haelt besser - for val_loss also?!\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.End:\n",
    "            # HOWTO access model from callback:\n",
    "            self.model.reset_states()\n",
    "\n",
    "# FITPAR['callbacks'] += [ hResetStatesCb() ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hTBoardTextCb: custom TensorBoard callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hTBoardTextCb v3b2: input {LogTag:LogStr,...} *OR* [(LogTag, LogStr),...]\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "\n",
    "hdict2list = lambda D: list( D.items() )\n",
    "\n",
    "class hTBoardTextCb(TensorBoard):\n",
    "    '''\n",
    "    USE:\n",
    "    callbacks += [ hTBoardTextCb(log_dir, MyLogs, **kwargs)]\n",
    "    with MyLogs = {'Tag1':'Text1',...} or [('Tag1','Text1'),...]\n",
    "    \n",
    "    This callback should *replace* any default callback of this sort:\n",
    "    callbacks += [keras.callbacks.TensorBoard(log_dir=TbDir,**kwargs)]\n",
    "    \n",
    "    Logs = [ ('Tag1', 'String1'), ('Tag2', 'String2'), ... ]\n",
    "    Could use an OrderedDict (from collections). But a simple pythetic \"dict\" does not preserve order.\n",
    "    \n",
    "    **{'batch_size':NB, 'histogram_freq':0, 'write_graph':False, 'write_images':False}\n",
    "    '''\n",
    "\n",
    "    def __init__(self, log_dir, MyLogs=None, **kwargs):\n",
    "        super().__init__(log_dir, **kwargs)\n",
    "        self.MyLogs = MyLogs\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        super().on_train_begin(logs)\n",
    "        if self.MyLogs is None:\n",
    "            return\n",
    "        try:\n",
    "            MyLogs = list(self.MyLogs.items()) # convert dict to list\n",
    "        except:\n",
    "            MyLogs = self.MyLogs\n",
    "\n",
    "        # Might consider: tf.summary.merge_all()\n",
    "        for TagText in MyLogs:\n",
    "            summary = tf.summary.text( TagText[0], tf.convert_to_tensor(TagText[1]) )\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/summary/text\n",
    "\n",
    "            with  tf.Session() as sess:\n",
    "                # No need for this?: self.writer = tf.summary.FileWriter('./Tensorboard', sess.graph)\n",
    "                s = sess.run(summary)\n",
    "                self.writer.add_summary(s)\n",
    "\n",
    "        # Do we need sth like this?: self.writer.close()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "python/3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h_DsRnn_Prep.ipynb [++++] Prep all data and train (one) model.\\ncalls h_DsRnn_ProcTb.ipynb and h_DsRnn_ExTb.ipynb\\n\\n* Select one parameter set from ProcTb = Par\\n* Load data into ExTb according to Par\\n* Create output folder\\n* Train and save results\\n\\nCONF: h_DsRnnAna_Prep.ipynb : Load, prep and save fMRI and phys. data in Gsig.mat files.\\n\\nPREC: h_DsRnn_Prep_v1b1.ipynb, h_DsRnn_v6b1.ipynb\\nAUTH: Hendrik.Mandelkow@nih.gov\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''h_DsRnn_Prep.ipynb [++++] Prep all data and train (one) model (per compute node).\n",
    "calls h_DsRnn_ProcTb.ipynb and h_DsRnn_ExTb.ipynb\n",
    "\n",
    "* Select one parameter set from ProcTb = Par\n",
    "* Load data into ExTb according to Par\n",
    "* Create output folder\n",
    "* Train and save results\n",
    "\n",
    "CONF: h_DsRnnAna_Prep_sba.ipynb : Load, prep and save fMRI and phys. data in Gsig.mat files.\n",
    "\n",
    "PREC: h_DsRnn_Prep_v1b1.ipynb, h_DsRnn_v6b1.ipynb\n",
    "AUTH: Hendrik.Mandelkow@nih.gov\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### TODO:\n",
    " - RNN:\n",
    "   - [ ] use corr.dist as loss\n",
    "   - [ ] use GRU (with lin. act.) as output\n",
    "   - [ ] use small layer just before output\n",
    " - [ ] train on diff(MRI) or piecewise const.\n",
    " - [ ] compare inter-scan, inter-session, inter-subj regression of Gsig\n",
    " - [ ] compare val.su and sections with w/o drop\n",
    " - [ ] export & plot layers\n",
    " - [x] sort ValSu by size\n",
    " - [ ] Dropout full TRs\n",
    " - [ ] Try different MaskVal eg 0 ((mean)) or const.\n",
    "   - [ ] read on LSTM missing values\n",
    " - [ ] Add more subjects\n",
    " - [ ] see TODO in h_DsRnn_v4a5.ipynb\n",
    "\n",
    "### DONE:\n",
    " - [x] Let's not save ExTb.\n",
    "       - PD df.to_hdf5 doesn't work bc of mixed data types\n",
    "       - Pickle is an option\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to record processing parameters (ProcPar) and other metadata?\n",
    "\n",
    " 1. Save ExTb, ProcTb, etc and re-load for analysis.\n",
    "    * Requires processing twice!\n",
    " 2. Create identical(!) ExTb, ProcTb for both training and analysis.\n",
    "    * Does not require separate processing.\n",
    "    * May still want to save for reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## sbatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------\n",
    "```bash\n",
    "# cd /data/mandelkowhc/Sleep/0302_Proc06_3su/sb_tmp\n",
    "\n",
    "cd ~/matlab/htools1/hpy # where to put slurm-*.out and script_1234.ipynb\n",
    "NBCOPT=\"--clear-output --ExecutePreprocessor.timeout=None --TemplateExporter.exclude_output=False --TemplateExporter.exclude_markdown=False --TemplateExporter.exclude_raw=True \"\n",
    "# --to notebook # ?!?\n",
    "\n",
    "NBCIN=~/matlab/htools1/hpy/h_DsRnn_Prep_sba4\n",
    "NARR=0-55\n",
    "echo \"#! /bin/bash\n",
    "jupyter nbconvert $NBCOPT --output ${NBCIN##*/}_\\$SLURM_ARRAY_JOB_ID-\\$SLURM_ARRAY_TASK_ID --execute $NBCIN.ipynb\" \\\n",
    "| sbatch -a $NARR -J ${NBCIN##*/} -t 36:00:00 --mem=64g -c 14 -p gpu --gres=gpu:p100:1,lscratch:32\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook htools_v1b.ipynb to python\n",
      "[NbConvertApp] Writing 34921 bytes to htools_v1b.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: +++ TEST +++ Using alternate MaskVal!\n",
      "   h_DsRnn_def.ipynb:15\n"
     ]
    }
   ],
   "source": [
    "%run h_DsRnn_def.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %matplotlib inline (static inline), notebook (dynamic inline?), auto (external, whichever qt,...), qt (external QT only)\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "hplotstyles();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false,
    "toc-nb-collapsed": false
   },
   "source": [
    "# Proc Parameters [`Par` / `ProcTb`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 471)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run h_DsRnn_ProcTb.ipynb\n",
    "ProcTb.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProcTb.filter(like='Su1Vx7').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcTb = ProcTb.filter(like='Su1Vx7Lxy')\n",
    "#ProcTb = ProcTb.filter(like='Su0Vs1Lxy')\n",
    "#ProcTb = ProcTb.filter(like='Su0Vs1Rper')\n",
    "#ProcTb = ProcTb.filter(like='Su1Vx7Rper')\n",
    "#ProcTb = ProcTb.filter(like='Su1Vx7Drop')\n",
    "#ProcTb = ProcTb.filter(like='Su0Vs1')\n",
    "#ProcTb = ProcTb.filter(like='Su0Vs1Rx1s')\n",
    "#ProcTb = ProcTb.filter(like='Su0Vs1Rs')\n",
    "# ProcTb = ProcTb.filter(like='Su1Vf')\n",
    "#ProcTb = ProcTb.filter(like='Vs1Xn')\n",
    "#ProcTb = ProcTb.filter(like='Vx1Xn')\n",
    "# ProcTb = ProcTb.filter(like='Su1Vr1Xn')\n",
    "# ProcTb = ProcTb.filter(like='Su1Vr3')\n",
    "# ProcTb = ProcTb.filter(like='Su1Vr3linNt30')\n",
    "# ProcTb = ProcTb.filter(like='Su1Vr3linTbat8')\n",
    "print('Run sbatch array with NARR=0-%u'%(ProcTb.shape[-1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcTb.T.reset_index(drop=True).head()\n",
    "# ProcTb.T.iloc[1]\n",
    "#ProcTb.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sbatch processing?\n",
    "Select Par = ProcPar.iloc[ :, SLURM_ARRAY_TASK_ID]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProcTb.filter(like='Su0Vs1').iloc[:,0] # NB: .filter works on *index* or *columns* only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assign Par.SbJob and .SbTask\n",
    "SbJob = os.environ.get('SLURM_ARRAY_JOB_ID')\n",
    "SbTask = os.environ.get('SLURM_ARRAY_TASK_ID')\n",
    "if SbJob is not None:\n",
    "    SbTask = int( SbTask )\n",
    "else:\n",
    "    SbJob = '000'\n",
    "    SbTask = 0\n",
    "\n",
    "# Par = ProcTb.iloc[:,SbTask] # +++ # want .to_dict()???\n",
    "Par = ProcTb.iloc[:,SbTask].to_dict() # +++ # want .to_dict()???\n",
    "Par = ddict(Par)\n",
    "Par.SbTask = SbTask\n",
    "Par.SbJob = SbJob\n",
    "# del SbJob, SbTask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcDir: set output folder\n",
    "Might as well do this 1st to check for existing.\n",
    "\n",
    "Let the ProcPath always be *DataDir/Proc/*. Rename older folders to Proc01,02,..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### OLD:\n",
    "# os.chdir( Par['DataDir']+'Proc03/' ) # ***\n",
    "ProcDir = None\n",
    "if Par['SbJob']: # sbatch session (else == None)\n",
    "    ProcDir = 'Km_%s/'%Par['SbJob']\n",
    "\n",
    "# HOWTO create a new unique directory with timestamp:\n",
    "while not ProcDir or os.path.exists(Par['DataDir']+ProcDir):\n",
    "    # ProcDir = 'Km_'+hdtimestr() # ***\n",
    "    ProcDir = 'Km_%s/'%htime64(None,True)\n",
    "\n",
    "Par.update(ProcDir=ProcDir)\n",
    "Par.ProcDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the ProcPath always be *DataDir/Proc/*. Rename older folders to Proc01,02,...\n",
    "os.makedirs( Par.DataDir+'Proc',exist_ok=True) # cleate and cd to DataDir/Proc\n",
    "os.chdir( Par.DataDir+'Proc' ) # ***\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Par.ProcDir = 'Km_%s-%02u_%s/'%( Par.SbJob[-3:], Par.SbTask, Par.ProcId )\n",
    "assert len( Par.ProcId.split('_') ) > 1, 'Oops?! Improper ProcId e.g. A3_Par1Par2...'\n",
    "Par.ProcDir = 'Km_%s/'%Par.ProcId # +++\n",
    "# print( Par.ProcDir )\n",
    "shutil.rmtree('Km_000_TEST',ignore_errors=True) # avoid error below\n",
    "os.makedirs( Par.ProcDir, exist_ok=False) # +++ Stop execution on existing dir\n",
    "# if SbJob is not None:\n",
    "#     assert not os.path.exists( Par.ProcDir ), 'Oops! Output folder ProcDir already exists!'\n",
    "print(os.getcwd()+'/'+Par.ProcDir)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Save ExTb and (Proc)Par to .h5\n",
    "ProcTb.T.to_hdf('ProcTb.h5','ProcTb',format='table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Load data - global signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: ProcId s are now expected to have this format: A12_Par1Par2Par3 (Nr_Description)\n",
    "Par.ProcId.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ExTb: Table (PD Df) of experiments / data files.\n",
    "%run ~/matlab/htools1/hpy/h_DsRnn_ExTb.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save ExTb:\n",
    " 1. don't save, recreate\n",
    "     * prob. best, since tra / val data may be different \n",
    " 2. pickle\n",
    " 3. split off data arrays\n",
    " 4. Xarray\n",
    " 5. list of dict\n",
    " 6. alternative ot PD table\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.iloc[0]\n",
    "#ExTb.iloc[0].McPar.shape\n",
    "#ExTb.iloc[0].Data.shape\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SecTb.to_hdf('SecTb_tmp.h5','SecTb')\n",
    "SecTb.drop(columns='Data').to_hdf('SecTb_xs.h5','SecTb')\n",
    "SecTb.Data[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.to_pickle('tmp.pkl.gz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "tmp = pd.read_pickle('tmp.pkl.gz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls -lh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mk TrainGen\n",
    " * AllData = train. and val. data\n",
    " * Data = training data\n",
    " * ValData = validation data\n",
    " \n",
    "Since TraGen and ValGen have to be the same size for a *stateful* RNN, it might be convenient simply to pass AllData along with separate masks for Data and ValData. (For added *security* one might zero training / validation data to guard against a buggy leak!?)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ValTag = False (training data), 'ValEx' (val.experiment), 'ValSu' (validation subject)\n",
    "ExTb.ValTag.tolist()\n",
    "# ExTb.ValSec[69]\n",
    "# SecTb.iloc[1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use SecTb instead?\n",
    "AllData = np.concatenate( ExTb.Data.tolist(), 0) # [Sec,t,Ch] # AllData = Train+Val data\n",
    "AllData_ValTag = np.concatenate( ExTb.ValSec.tolist(), 0) # [Sec] boolean index\n",
    "\n",
    "if Par.ValFrac: # +++\n",
    "    # AllData_ValTag[ AllData_ValTag < 1 ] = AllData_ValTag[ AllData_ValTag < 1 ] >= (1-Par.ValTag) # +++\n",
    "    AllData_ValTag = np.where( AllData_ValTag<1, AllData_ValTag>=(1-Par.ValFrac), AllData_ValTag )\n",
    "\n",
    "AllData_ValTag = AllData_ValTag.astype(int)\n",
    "\n",
    "#AllData = np.concatenate( SecTb.Data.tolist(), 0)\n",
    "#AllData_ValTag = np.concatenate( SecTb.ValSec.tolist(), 0)\n",
    "\n",
    "AllData = AllData[...,[0,1,-2,-1]]\n",
    "AllData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Par.RandPerm:\n",
    "    warn('+++ Randomized control experiment!')\n",
    "    # AllData = np.concatenate( [ np.random.permutation( AllData[...,:2]), AllData[...,2:] ], -1) # simpler!\n",
    "    # np.random.shuffle( AllData[...,:2]) # simplest?!\n",
    "\n",
    "    tmp = np.random.permutation( AllData.shape[0])\n",
    "    while np.any( tmp == np.arange(tmp.size)):\n",
    "        tmp = np.random.permutation(tmp)\n",
    "\n",
    "    AllData = np.concatenate( [ AllData[tmp,:,:2], AllData[...,2:]], -1)\n",
    "    AllData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ValData = AllData[ AllData_ValTag != 0 ]\n",
    "#ValData = np.copy(AllData[AllData_ValTag != 0]) # +++ copy necessary?\n",
    "Data = AllData[ AllData_ValTag == 0 ] # +++ training data\n",
    "#> Data = AllData[ AllData_ValTag < (1-Par.ValFrac) ] # +++ training data\n",
    "#> ValData = AllData[AllData_ValTag != 0] # +++ validation data\n",
    "#> ValData = AllData[AllData_ValTag == Par.ValTag] # +++ validation data 1,2,3 = ValMiddle, ValEx1, ValSu\n",
    "#< ValData = AllData[ np.any(AllData_ValTag.reshape(-1,1)==np.array(Par.ValTag).reshape(1,-1),1)]\n",
    "ValData = AllData[ np.any( np.c_[AllData_ValTag] == np.r_[Par.ValTag],1) ] # +++\n",
    "\n",
    "assert Data.size>0, 'Oops! No training data!'\n",
    "assert ValData.size>0, 'Oops! No ValData!'\n",
    "\n",
    "Data.shape, ValData.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# (ExTb.ValTag == 'ValSu').to_numpy(int) + (ExTb.ValTag == 'ValEx').to_numpy(int)\n",
    "# (ExTb.ValTag == 'ValSu').to_numpy(int) + ExTb.ValTag.to_numpy(bool).astype(int)\n",
    "ExTb.apply( lambda x: x.ValSec.astype(int) + (x.ValTag != False), 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = Par['NT']\n",
    "NY = 1 # ***\n",
    "NX = Data.shape[-1]-NY\n",
    "assert (Par.NX==NX) and (Par.NY==NY), 'Oops!'\n",
    "NB = Data.shape[0]\n",
    "# TRFS = Par.TRFS\n",
    "TGEN = {}\n",
    "TGEN['Drop'] = Par.Drop\n",
    "TGEN['Xtrafo'] = Par.Xtrafo\n",
    "#< TGEN['RandXlead'] = Par.RandXlead\n",
    "# TrainGen = hBatchSeq1y( Data.reshape(-1,Data.shape[-1]), NT=NT, NX=NX, NB=NY*NB, Drop=0)\n",
    "TrainGen = hBatchSeq1y( Data.reshape(-1,Data.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **TGEN)\n",
    "TrainGen.RandXlead = Par.RandXlead\n",
    "TrainGen.RandYscale = Par.RandYscale\n",
    "TrainGen[0][0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TrainGen.setYscale(1.0)\n",
    "TrainGen.RandYscale = 0.5\n",
    "TrainGen.on_epoch_end()\n",
    "print( TrainGen.Yscale )\n",
    "TrainGen.Data\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TrainGen.setXlead(2)\n",
    "print( TrainGen.Xlead )\n",
    "TrainGen.Data\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TrainGen.RandXlead = 20\n",
    "TrainGen.on_epoch_end()\n",
    "# TrainGen.setXlead(0)\n",
    "print( TrainGen.Xlead )\n",
    "TrainGen.Data\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hrange = lambda n,dx=1,x0=0: range(x0,x0+n*dx,dx) # = [0:n-1]*dx+x0\n",
    "\n",
    "hplotstyles()\n",
    "# tmp = TrainGen[0][0][hrange(10,1,10),:,:]\n",
    "tmp = TrainGen[0][0][10:20,:,:]\n",
    "plt.plot(tmp.reshape(-1,tmp.shape[-1]))\n",
    "plt.plot(tmp[...,-1].reshape(-1,1),'r.')\n",
    "plt.plot(TrainGen[0][1][10:20,:,-1].reshape(-1,1),'g.')\n",
    "plt.xticks(np.arange(tmp.shape[0])*tmp.shape[1])\n",
    "plt.grid('x')\n",
    "#plt.ylim(-4,4)\n",
    "#plt.xlim(250,350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Need to append 0s to make ValData the same (batch) size as Data for stateful RNN model.\n",
    "# VGEN['Mask'] shall be used to mask the excess ValData.\n",
    "VGEN = {}\n",
    "VGEN['Drop'] = Par.ValDrop\n",
    "VGEN['Xtrafo'] = Par.Xtrafo\n",
    "# Need to pass a sample_weights, in order to have ValidGen return (X,Y,W).\n",
    "# Weights used here to mask smaller val.batch size.\n",
    "VGEN['Mask'] = (np.arange(Data.shape[0])<ValData.shape[0]).astype(int) # mask of ones. see above\n",
    "# ValData = np.copy(ValData)\n",
    "# ValData.resize(Data.shape) # append zeros. Doesn't work. Stupid Python.\n",
    "tmp = ValData\n",
    "ValData = 0*Data\n",
    "ValData[:tmp.shape[0]] = tmp\n",
    "# ValData = np.resize( ValData, Data.shape) # repeat array\n",
    "ValData.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValidGen = hBatchSeq1y( ValData.reshape(-1,ValData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, Drop=Par.ValDrop, Mask=Mask)\n",
    "ValidGen = hBatchSeq1y( ValData.reshape(-1,ValData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **VGEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "## mk ValGen\n",
    "Since TraGen and ValGen have to be the same size for a *stateful* RNN, it might be convenient simply to pass AllData along with separate masks for Data and ValData. (For added *security* one might zero training / validation data to guard against a buggy leak!?)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NT = Par['NT']\n",
    "NY = 1 # ***\n",
    "NX = Data.shape[-1]-NY\n",
    "assert (Par.NX==NX) and (Par.NY==NY), 'Oops!'\n",
    "NB = Data.shape[0]\n",
    "# TRFS = Par.TRFS\n",
    "TGEN = {}\n",
    "TGEN['Drop'] = Par.Drop\n",
    "TGEN['Xtrafo'] = Par.Xtrafo\n",
    "TGEN['Mask'] = ( AllData_ValTag == 0 ).astype(int)\n",
    "\n",
    "TrainGen = hBatchSeq1y( AllData.reshape(-1,AllData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **TGEN)\n",
    "TrainGen[0][0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "VGEN = TGEN\n",
    "VGEN['Mask'] = ( TGEN['Mask']==0 ).astype(int) # mask of ones. see above\n",
    "ValidGen = hBatchSeq1y( AllData.reshape(-1,AllData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **VGEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRFS = Par.TRFS\n",
    "BPE,NT,NY = TrainGen[0][1].shape\n",
    "NX = TrainGen[0][0].shape[-1]-NY\n",
    "NB = BPE//NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB, BPE, NT, NX, NY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ exec(n+'=Par[\"%s\"]'%n ) for n in 'TR Fs'.split() ] # Why not work?!?\n",
    "# TR,Fs,NT,NX,NY,NB = [ Par[n] for n in 'TR,Fs,NT,NX,NY,NB'.split(',') ]\n",
    "if not isinstance(NT,int) or (NT < TRFS): # NT given in sec\n",
    "    NT = int(NT//TR*TR*Fs) # each seq.length in samples (Fs)\n",
    "    Par.update(NT=NT)\n",
    "\n",
    "# Prob.WRONG! assert Data.shape[1]>=NX+NB, 'Oops, not enough data channels!'\n",
    "assert Data.shape[1]>=NX+NY, 'Oops, not enough data channels!'\n",
    "\n",
    "keras.backend.clear_session()\n",
    "#< Par['RnnArg'][0] = []\n",
    "# RNN = mkRNN( **Par['RnnArg'] ) # +++ ***\n",
    "if True:\n",
    "    # RNN = mkRNN( [NX+1,'G',2**6,2**6,2**5,'D',1], NT, BPE) # +++ ***\n",
    "    Par['RnnNio'][0] = NX+1\n",
    "    # RNN = mkRNN( Par['RnnNio'], NT, BPE) # +++ ***\n",
    "    Par['RnnArg'] = { 'Nio':Par['RnnNio'], 'Nsteps':NT, 'Nbatch':BPE}\n",
    "    RNN = mkRNN( **Par['RnnArg'] ) # +++ ***\n",
    "else:\n",
    "    Par['RnnArg'] = { 'Nio':Par['RnnNio'], 'Nsteps':NT, 'Nbatch':BPE, 'stateful':False}\n",
    "    # RNN = mkRNN( [NX+1,'G',2**6,2**6,2**5,'D',1], NT, BPE, stateful=False) # +++ ***\n",
    "    RNN = mkRNN( **Par['RnnArg'] ) # +++ ***\n",
    "    warn('+++ TEST TEST TEST +++ stateful = False!!!')\n",
    "\n",
    "# Par['RnnArg'] = RNN.mkRNNargs # FIXIT: Broke with tf.keras\n",
    "\n",
    "if False:\n",
    "    # HOWTO change Keras model by recompiling:\n",
    "    # HOWTO use tf.keras.losses.CosineSimilarity():\n",
    "    RNN.compile( loss=tf.keras.losses.CosineSimilarity(axis=-2), optimizer='adam', metrics=[ KCustoms['hWRVF'] ]) # +++\n",
    "\n",
    "# RNN.mkRNNargs = [[NX+1,'G',2**6,2**6,2**5,'D',1], NT, NY*NB]\n",
    "RNN.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RNNp = mkRNN([NX+1,'G',2**6,2**6,2**5,'D',1], NT, 1) # prediction model\n",
    "RNNp = mkRNN([NX+1,'G',2**6,2**6,2**5,'D',1], NT, NY) # prediction model\n",
    "RNNp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: Par['Ngpus'] = os.environ.get('SLURM_STEP_GPUS').count(',')+1\n",
    "except: Par['Ngpus'] = 0\n",
    "if Par['Ngpus']:\n",
    "    warn('You *should* be using Keras..multi_gpu_model()!!!')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# WARNING: This will not work with STATEFUL RNNs!\n",
    "# WARNING: Cannot use RNNX.save .save_weights?! Must use RNN.save instead.\n",
    "# RNNX = keras.utils.multi_gpu_model( RNN ) # may require arg: cpu_relocation=True\n",
    "RNNX = keras.utils.multi_gpu_model( RNN, gpus=Par['Ngpus'], cpu_merge=True, cpu_relocation=True ) # may require arg: cpu_relocation=True\n",
    "RNNX.compile(**{ n:RNN.__dict__[n] for n in ['loss','optimizer','metrics'] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make data generators: TrainGen, ValidGen, (TestGen)\n",
    "### The validation batch size problem\n",
    "In Keras a *stateful* RNN *requires* a fixed training batch size (`NB*NY` is not None). This in turn mandates that both TrainGen and ValidGen have the same `NB*NY`. For continuous (stateful) prediction (NB=1) we must define a separate model (RNNp) *and* data generator (TestGen)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NOTE: TrainGen and ValidGen need to have the same batch_size (NY*NB).\n",
    "# NOTE: The ValidGen used during training must have the same input_shape as the TrainGen (inconveniently).\n",
    "# TODO: For prediction, one might later use NB=1 and maybe NY=1.\n",
    "\n",
    "ValFrac = Par['ValFrac']\n",
    "assert ( 0<= ValFrac <=1 ), 'Oops?!'\n",
    "if False:\n",
    "    TrainGen = hBatchSeq1y(Data[:-int(Data.shape[0]*ValFrac),:NX+NY], NT=NT, NX=NX, NB=NB, Mask=Par['MaskVal'])\n",
    "    ValidGen = hBatchSeq1y(Data[-int(Data.shape[0]*ValFrac):,:NX+NY], NT=NT, NX=NX, NB=NB, Mask=Par['MaskVal'])\n",
    "elif True:\n",
    "    warn('+ Using interleaved val.data!')\n",
    "    TrainGen = hBatchSeq1y(Data[:,:NX+NY], NT=NT, NX=NX, NB=NB, ValFrac=Par['ValFrac'])\n",
    "    ValidGen = hBatchSeq1y(Data[:,:NX+NY], NT=NT, NX=NX, NB=NB, ValFrac=-Par['ValFrac'])\n",
    "else:\n",
    "    # TrainGen = hBatchSeq1y(Data[:,[0,1,3]], NT=NT, NX=NX, NB=NB)\n",
    "    TrainGen = hBatchSeq1y(Data[:-int(Data.shape[0]*ValFrac),:NX+NY], NT=NT, NX=NX, NB=NB)\n",
    "    ValidGen = hBatchSeq1y(Data[-int(Data.shape[0]*ValFrac):,:NX+NY], NT=NT, NX=NX, NB=NB)\n",
    "# Use more voxels NY instead of NB:\n",
    "# ValidGen = hBatchSeq1y(Data[-int(Data.shape[0]*ValFrac):,:NX+NY*NB], NT=NT, NX=NX, NB=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "*******************************************************************\n",
    "# ProcDir: set output folder"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.makedirs( Par.DataDir+'Proc',exist_ok=True)\n",
    "os.chdir( Par.DataDir+'Proc' ) # ***\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# os.chdir( Par['DataDir']+'Proc03/' ) # ***\n",
    "\n",
    "ProcDir = None\n",
    "if Par['SbJob']: # sbatch session (else == None)\n",
    "    ProcDir = 'Km_%s/'%Par['SbJob']\n",
    "\n",
    "while not ProcDir or os.path.exists(Par['DataDir']+ProcDir):\n",
    "    # ProcDir = 'Km_'+hdtimestr() # ***\n",
    "    ProcDir = 'Km_%s/'%htime64(None,True)\n",
    "\n",
    "Par.update(ProcDir=ProcDir)\n",
    "Par.ProcDir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProcDir = 'Km_JA4IMo/'\n",
    "Par.update(ProcDir=ProcDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks (FITPAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#< os.makedirs( Par.DataDir+'Proc',exist_ok=True)\n",
    "os.chdir( Par.DataDir+'Proc' ) # *** Just to be sure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "ProcDir = Par.ProcDir\n",
    "FITPAR = {'callbacks':[]}\n",
    "# FITPAR['epochs'] = 1\n",
    "# FITPAR['validation_split'] = 0.2 # v.split not for generator\n",
    "# https://keras.io/callbacks/#modelcheckpoint\n",
    "# tmp = re.sub(r'[-_.][^/\\\\]+$',r'-e{epoch:02d}-{loss:.2f}.h5',ModelFile)\n",
    "#> tmp = re.sub(r'\\.[^/\\\\]+$',r'_Ep{epoch:03d}.h5',ModelFile)\n",
    "# tmp = 'tmp.h5'\n",
    "# keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "FITPAR['callbacks'] += [keras.callbacks.ModelCheckpoint(ProcDir+'model.h5', monitor='loss')]; # print(ModelFile)\n",
    "FITPAR['callbacks'] += [keras.callbacks.ModelCheckpoint(ProcDir+'model_BestTra.h5', monitor='loss',save_best_only=True)]\n",
    "FITPAR['callbacks'] += [keras.callbacks.ModelCheckpoint(ProcDir+'model_BestVal.h5', monitor='val_loss',save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs( ProcDir+'weights/',exist_ok=True)\n",
    "FITPAR['callbacks'] += [keras.callbacks.ModelCheckpoint(ProcDir+'weights/weights_E{epoch:05d}.h5', monitor='loss',save_best_only=True,save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITPAR['callbacks'] += [ hResetStatesCb() ] # False/True on_epoch_begin / _end\n",
    "# KmInfoStr.append('hResetStatesCb(False): on_epoch_begin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITPAR.update({'validation_data': ValidGen, 'validation_steps': 32, 'validation_freq': 8})\n",
    "# NOTE: val_freq requires Keras >2.2?!\n",
    "# FITPAR.update({'validation_data': ValidGen, 'validation_steps': 32})\n",
    "FITPAR.update({'validation_data': ValidGen, 'validation_steps':None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Many optimizers include adaptive LR. Therefore LR schedule may be less irrelevant.\n",
    "# TODO: is min_delta dependent on the abs scale of loss?\n",
    "# NOTE: ?!? Reduce LR, if loss hasn't fallen by at least min_delta over the past \"patience\" epochs?!\n",
    "# Basically, require average dLoss/dt > min_delta / patience\n",
    "# keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "# FITPAR['callbacks'] += [keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8, patience=10, min_lr=0.00001, verbose=1)]\n",
    "FITPAR['callbacks'] += [keras.callbacks.ReduceLROnPlateau(monitor='hWRVF', min_delta=0.005, patience=8, factor=0.8, min_lr=0.00001, cooldown=0, verbose=1)]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "FITPAR['callbacks'] += [keras.callbacks.callbacks.EarlyStopping(monitor='val_hWRVF', baseline=3.0, mode='min', verbose=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITPAR['callbacks'] += [keras.callbacks.CSVLogger(ProcDir+'train_log.csv',separator=',',append=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HOWTO list newest files in folder:\n",
    "# os.listdir() os.scandir()\n",
    "# Fname = max(glob.iglob('*.h5'), key=os.path.getctime)\n",
    "Fname = max(glob.iglob('Km*/model.h5'), key=os.path.getctime)\n",
    "#Fname = sorted(glob.iglob('*.h5'), key=os.path.getctime, reverse=True)[0]\n",
    "Fname"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LoadFile = None\n",
    "LoadFile = Fname\n",
    "#LoadFile = 'G64G64G32D1_XRZ33w.h5'\n",
    "#LoadFile = 'G64G64G32D1_XRlHAw.h5'\n",
    "# LoadFile = 'Km20190714-014524_Lval0p66_Ep91.h5'\n",
    "# LoadFile = 'Km_20190714-130723/model_Lval.h5'\n",
    "LoadFile = 'Km_20190714-163804/model_Lval.h5'\n",
    "LoadFile = 'Km_20190714-175543/model_Lval.h5'\n",
    "LoadFile = 'Km_20190714-234856/model_Lval.h5'\n",
    "LoadFile = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Problem with custom loss hWeightedLoss. Load weights instead.\n",
    "if LoadFile is not None:\n",
    "    RNN.load_weights(LoadFile) # ***\n",
    "    #SaveFile = LoadFile # *** Overwrite?!?\n",
    "print(SaveFile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras import backend as K\n",
    "keras.__version__ # 2.0.2\n",
    "# HOWTO get set model learning rate manually:\n",
    "print(K.get_value(RNN.optimizer.lr),end=' -> ')\n",
    "K.set_value(RNN.optimizer.lr, 0.0005)\n",
    "print(K.get_value(RNN.optimizer.lr))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.chdir( Par.DataDir+'Proc03/' ) # *** New DataDir?!\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HOWTO list newest files in folder:\n",
    "# os.listdir() os.scandir()\n",
    "# Fname = max(glob.iglob('Km*/model.h5'), key=os.path.getctime)\n",
    "Fname = sorted(glob.iglob('Km*/model.h5'), key=os.path.getctime, reverse=False) #[0]\n",
    "Fname"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProcDir = hFpath(Fname[-1])\n",
    "ProcDir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls Km_JA4IMo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# with keras.utils.CustomObjectScope({'AttentionLayer': AttentionLayer}):\n",
    "RNN = keras.models.load_model(ProcDir+'model_BestVal.h5', custom_objects=KCustoms)\n",
    "# RNN.load_weights(LoadFile) # ***\n",
    "#SaveFile = LoadFile # *** Overwrite?!?\n",
    "#print(SaveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************\n",
    "# Save model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(Par.DataDir+'Proc/')\n",
    "# This should be done before.\n",
    "# os.makedirs(ProcDir, exist_ok=False) # +++ error if exist, no overwrite!\n",
    "print(os.getcwd()+'/'+ProcDir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#< import hdf5storage as hdf5\n",
    "# truncate_existing=True # = overwrite (don't append)\n",
    "KmInfo = {} if 'KmInfo' not in locals() else KmInfo\n",
    "# KmInfPar.update({'SaveFile':SaveFile, 'NiiFile':NiiFile, 'NiiMask':NiiMask, 'AcqFile':AcqFile, \\\n",
    "#          'NX':NX, 'NY':NY, 'NT':NT, 'NB':NB, 'LoadFile':LoadFile, 'Xmask':Xmask })\n",
    "KmInfPar.update({'SaveFile':SaveFile, 'NiiFile':NiiFile, 'AcqFile':AcqFile, 'TRFS':TRFS, \\\n",
    "         'NX':NX, 'NY':NY, 'NT':NT, 'NB':NB, 'LoadFile':LoadFile, 'Xmask':Xmask, 'ValFrac':ValFrac })\n",
    "\n",
    "# KmInfPar.update({'RIC':RIC, 'Xtv':Xtv, 'Data':Data, 'Mask':Mask.dataobj})\n",
    "\n",
    "# hdf5.savemat(SaveFile.replace('.h5','.mat'), KmInfo, truncate_existing=True)\n",
    "hdf5.savemat(ProcDir+'metapar.mat', KmInfo, truncate_existing=True)\n",
    "# KmInfo = hdf5.loadmat(SaveFile.replace('.h5','.mat'))\n",
    "\n",
    "# HDF5OPT = {'store_python_metadata':True, 'matlab_compatible':True}\n",
    "# hdf5.writes(KmInfo, SaveFile.replace('.h5','.mat'), **HDF5OPT)\n",
    "# hdf5storage.read(path='/a', filename='data_typeinfPar.mat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save metaparameters (Proc)Par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5.savemat( ProcDir+'metapar.mat', Par, truncate_existing=True)\n",
    "#< scipy.io.savemat( ProcDir+'metapar.mat', Par) # cannot store None?!?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open( ProcDir+'metapar.json', 'w') as file:\n",
    "    json.dump( Par, file, indent=4, sort_keys=True, separators=(', ',': '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open( ProcDir+'metapar.yaml', 'w') as file:\n",
    "    yaml.dump( dict(Par), file)\n",
    "    # yaml.dump( Par, file, indent=4, sort_keys=True, separators=(', ',': '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save RNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( ProcDir+'model.json', 'w') as file:\n",
    "    file.write( RNN.to_json() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save training and validation data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "assert ProcDir[-1]=='/', 'Hoppla!?!'\n",
    "hdf5.savemat(ProcDir+'data.mat', dict(Par), truncate_existing=True)\n",
    "# hdf5.savemat(ProcDir+'data.mat', {'Xtv':Xtv,'Data':Data[:,:NX+1]}, truncate_existing=False)\n",
    "hdf5.savemat(ProcDir+'data.mat', {'Data':Data,'ValData':ValData,'ValMask':VGEN['Mask']}, truncate_existing=False)\n",
    "# hdf5.savemat(ProcDir+'data.mat', {'Xtv':Xtv}, truncate_existing=False)\n",
    "#tmp = csr_matrix(Data.reshape(-1,Data.shape[-1]))\n",
    "#hdf5.savemat(ProcDir+'data.mat', {'DataCSR':(tmp.data,tmp.indices,tmp.indptr)}, truncate_existing=False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scipy\n",
    "scipy.sparse.save_npz(ProcDir+'data.npz',csr_matrix(Data.reshape(-1,Data.shape[-1])))\n",
    "# tmp = scipy.sparse.load_npz(ProcDir+'data.npz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Save ExTb to .h5\n",
    "ExTb.to_hdf( ProcDir+'ExTb.h5','ExTb',format='table') # Pandas BUG! must use format='table'\n",
    "pd.Series(Par).to_hdf( ProcDir+'ExTb.h5','Par',format='table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save SecTb to .h5\n",
    "#< ProcDir = ''\n",
    "SecTb['Data0'] = SecTb['Data'].apply(lambda x: x.flat[0])\n",
    "SecTb.drop(columns='Data').to_hdf( ProcDir+'SecTb_xs.h5','SecTb') # ?!? Pandas BUG! must use format='table'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save TensorBoard training stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use default TBoard callback:\n",
    "FITPAR['callbacks'] += [ TensorBoard( './'+ProcDir ) ]\n",
    "# FIXIT: Sadly, not compatible with Keras anymore."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Use customized TBoard CB for logging arbitrary strings:\n",
    "assert 'tensorboard' not in str(type(FITPAR[-1])).lower(), \"Oops! Don't use multiple TBoard callbacks.\"\n",
    "# NOTE: Each TBoard callback writes it's own events.* file. Use only one TB callback!\n",
    "# https://keras.io/callbacks/#tensorboard\n",
    "# keras.callbacks.tensorboard_v1.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, \\\n",
    "# embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "LogInfo = {}\n",
    "# LogInfo = { n:m for n,m in KmInfPar.items() if isinstance(m,str)}\n",
    "# LogInfo['NXYB'] = str((NX,NY,NB))\n",
    "# LogInfo['KmInfoStr'] = '\\n'.join(KmInfoStr)\n",
    "#< FITPAR['callbacks'] += [ hTBoardTextCb( TbDir, LogInfo) ]\n",
    "FITPAR['callbacks'] += [ hTBoardTextCb( './'+ProcDir, LogInfo) ]\n",
    "# FITPAR['callbacks'] += [ hTBoardTextCb( './'+ProcDir, LogInfo, histogram_freq=1, write_grads=True, write_images=True ) ] # requires val.data *not* generator!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook $ProcDir/h_DsRnn_tmp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = RNN.fit_generator(TrainGen, initial_epoch=n, epochs=n+1, **FITPAR) # initial / final epochs\n",
    "hist = RNN.fit_generator(TrainGen, epochs=Par['Epochs'], **FITPAR) # initial / final epochs\n",
    "# NOTE: hist = model.fit() = model.history.history # which is replaced at every call to fit()\n",
    "# If hist = fit() is interrupted, hist is empty, but model.history.history still exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model successively on ValSu + ValEx1 + ValMiddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************************************************\n",
    "# Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = Par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session() # necessary?!?\n",
    "print('+ Load '+par['ProcDir']+'model_BestVal.h5')\n",
    "if False:\n",
    "    par.update( RNN = keras.models.load_model( par['ProcDir']+'model_BestVal.h5', custom_objects=KCustoms))\n",
    "elif False:\n",
    "    with open(ProcDir+'model.json','r') as json_file:\n",
    "        RNN = keras.models.model_from_json( json_file.read() )\n",
    "    RNN.load_weights(ProcDir+'model_BestVal.h5')\n",
    "else:\n",
    "    RNN.load_weights(ProcDir+'model_BestVal.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on val.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict yh and append x,y,yh to par[]\n",
    "par = { 'x': ValidGen.getX(), 'y': ValidGen.getY(), 'yh': ValidGen.predict( RNN ) }\n",
    "\n",
    "## Reshape x,y,yh to [Sec,t,Ch] and discard dummy channels ValidGen.Mask==0\n",
    "tmp = ( ValidGen.NB, len(ValidGen)*ValidGen.NT, ValidGen.NY )\n",
    "par['x'] = par['x'].reshape( *tmp[:-1], -1)[ValidGen.Mask>0,...]\n",
    "par['y'] = par['y'].reshape( *tmp )[ValidGen.Mask>0,...]\n",
    "par['yh'] = par['yh'].reshape( *tmp )[ValidGen.Mask>0,...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save val. results as ???\n",
    "EvalData, ValOut, ProcData?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = SecTb.drop('Data').to_dict\n",
    "par['SecTb'] = SecTb.to_dict('list')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SecTb[['Su','Se','Ex','Sec','ValSec','Data0']].to_numpy()\n",
    "par['SecNp'] = SecTb[['Su','Se','Ex','Sec','ValSec']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ProcDir[-1]=='/', 'Hoppla!?!'\n",
    "hdf5.savemat(ProcDir+'EvalData.mat', dict(par), truncate_existing=True, format='5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Predict internal layer activations\n",
    "see `h_DsRnnAna_v6b2_sba` RNNa.pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over weights and val. sets\n",
    "## TODO:\n",
    " - [ ] use .predict instead of .evaluate?!\n",
    " - [ ] use masked ValData with original model?!\n",
    " - [ ] use mkRNN from scratch?!\n",
    " - [ ] forget about this and simply retrain?!\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ProcDir = 'Km_Vs1_DropLast50vVsu003/'\n",
    "# AllData_ValTag = AllDataVal\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "Fnames = sorted(glob.glob(ProcDir+'weights/weights_E*.h5'))\n",
    "ValLosses = []\n",
    "VGEN.pop('Mask',None) # HOWTO delete dict in place without error\n",
    "K.clear_session()\n",
    "for Fname in Fnames:\n",
    "    # print(Fname)\n",
    "    ep = int(re.findall('_E\\d+',Fname)[-1][2:])\n",
    "    RNN.load_weights(Fname)\n",
    "    for n in filter( None, np.unique(AllData_ValTag)):\n",
    "        # ValData = AllData[ np.any( np.c_[AllData_ValTag] == np.r_[Par.ValTag],1) ]\n",
    "        ValData = AllData[ np.any( np.c_[AllData_ValTag]==n, 1) ]\n",
    "        #< ValidGen = hBatchSeq1y( ValData.reshape(-1,ValData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **VGEN)\n",
    "        ValidGen = hBatchSeq1y( ValData.reshape(-1,ValData.shape[-1]), \\\n",
    "                               NT=NT, NX=ValData.shape[-1]-1, NB=1, **VGEN)\n",
    "        if len(ValLosses)==0:\n",
    "            # K.clear_session()\n",
    "            RNN = ValidGen.reshapeInput(RNN)\n",
    "        # par = { 'x': ValidGen.getX(), 'y': ValidGen.getY(), 'yh': ValidGen.predict( RNN ) }\n",
    "        # assert False: 'Need opt and metrics to compile this model...'\n",
    "        ValLosses.append( ValidGen.evaluate( RNN ).update( Ep=ep, ValTag=n ) )\n",
    "\n",
    "    print('.',end='')\n",
    "\n",
    "# RNN.load_weights\n",
    "# for ValidGen = ...\n",
    "# ValData = AllData[ np.any( np.c_[AllData_ValTag] == np.r_[Par.ValTag],1) ]\n",
    "# ValidGen = hBatchSeq1y( ValData.reshape(-1,ValData.shape[-1]), NT=NT, NX=NX, NB=NY*NB, **VGEN)\n",
    "\n",
    "# M[Ep,Vset,Metric] = ValidGen.eval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''loadData.ipynb : Make Pandas table of data to be called from h_DsRnn_Prep_sba.ipynb\n",
    "\n",
    "PREC: h_DsRnn_ExTb.ipynb\n",
    "AUTH: Hendrik.Mandelkow@gmail.com\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\",16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Requirements - for testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %matplotlib inline (static inline), notebook (dynamic inline?), auto (external, whichever qt,...), qt (external QT only)\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "hplotstyles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: ( mkRNN )\n",
    "except:\n",
    "    # %run ./h_DsRnn_def.ipynb\n",
    "    %run ./modelDef.ipynb\n",
    "\n",
    "try: ( ProcTb, Par )\n",
    "except:\n",
    "    # %run ./h_DsRnn_ProcTb.ipynb\n",
    "    %run ./trainPars.ipynb\n",
    "ProcTb.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mkExTb: 1st find all experiments in DataDir\n",
    " 1. Start with a set of metapars (created in h_DsRnn_Prep)\n",
    " 2. Find all the Par.NiiFile in Par.DataDir and create a pd.DataFrame column - one row per experiment (dataset).\n",
    " 3. Populate other columns with derived metadata:\n",
    "    - NiiSz\n",
    "    - ProcDir\n",
    "    - ExId\n",
    "    - Su\n",
    "    - Se\n",
    "    - Ex\n",
    "    - ExRank ???\n",
    "    - ValRank ???\n",
    "    - ValTag : (str) 'ValEx'\n",
    "    - Sc : section ???\n",
    "    - HypnoFile : Sleep score file\n",
    "    - VmrkFile : EEG marker file\n",
    "    - ...\n",
    " 4. Load and preproc data to populate columns:\n",
    "    - Data\n",
    "    - ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Par = ddict(Par) # +++ Par was created in h_DsRnn_Prep\n",
    "os.chdir(Par.DataDir)\n",
    "print(os.getcwd())\n",
    "#< Fname = hsys('find Su%s -iname \"%s\"'%( Par.SuId, Par.NiiFile) ) # MoCorr + MoReg\n",
    "Fname = hsys('find sub-%s -iname \"%s\"'%( Par.SuId, Par.NiiFile) ) # MoCorr + MoReg\n",
    "# print(Fname[:10],sep='\\n')\n",
    "print('\\n'.join(Fname[:10]))\n",
    "print('...%u'%len(Fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExTb = pd.DataFrame(Fname,columns=['NiiFile'])\n",
    "ExTb['NiiSz'] = ExTb.NiiFile.apply(os.path.getsize) # +++ size in bytes\n",
    "ExTb['NiiSz'] = ExTb['NiiFile'].apply( lambda x: niba.load(x).shape[-1] ) # +++ size in volumens, much slower!\n",
    "ExTb = ExTb.sort_values('NiiSz').reset_index(drop=True)\n",
    "# ExTb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mk ProcDir + NiiFile\n",
    "#< ExTb['ProcDir'] = Tb.NiiFile.apply(hFpath)\n",
    "# HOWTO extract (multiple) regex groups and store them in new columns:\n",
    "ExTb[['ProcDir','NiiFile']] = ExTb.NiiFile.str.extract(r'(.*/)([^/]+)') # 2 captures for 2 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# HOWTO use Pandas rename which requires a mapper function or a dict-like:\n",
    "tmp = ExTb.NiiFile.str.extract(r'(.*/)([^/]+)').rename(columns={0:'ProcDir',1:'NiiFile'})\n",
    "tmp = ExTb.NiiFile.str.extract(r'(.*/)([^/]+)').rename(columns=dict(zip(range(2),['ProcDir','NiiFile'])))\n",
    "tmp = ExTb.NiiFile.str.extract(r'(.*/)([^/]+)').rename(columns=lambda n:('ProcDir','NiiFile')[n])\n",
    "tmp = ExTb.NiiFile.str.extract(r'(?P<ProcDir>.*/)(?P<NiiFile>[^/]+)')\n",
    "tmp\n",
    "ExTb[['ProcDir','NiiFile']] = tmp\n",
    "# pd.concat([tmp,ExTb],1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### mk Su,Se,Ex,ExId (='20180706_0423' eg)\n",
    "# HOWTO extract (multiple) regex groups and store them in new columns:\n",
    "# tmp = ExTb.ProcDir.str.extract(r'^Su(\\d+)/Se(\\d+)/Ex(\\d+)/([\\d_]+).results')\n",
    "# tmp = ExTb.ProcDir.str.extract(r'^Su(\\d+)/Se(\\d+)/Ex(\\d+)/').astype(int).rename(columns={0:'Su',1:'Se',2:'Ex'})\n",
    "# tmp = ExTb.ProcDir.str.extract(r'^Su(\\d+)/Se(\\d+)/Ex(\\d+)/').astype(int); tmp.columns = ['Su','Se','Ex']\n",
    "# tmp = ExTb.ProcDir.str.extract(r'^Su(?P<Su>\\d+)/Se(?P<Se>\\d+)/Ex(?P<Ex>\\d+)/').astype(int)\n",
    "tmp = ExTb.ProcDir.str.extract(r'^sub-(?P<Su>\\d+)/ses-(?P<Se>\\d+)/run-(?P<Ex>[\\d_]+)/').astype(int)\n",
    "tmp['ExId'] = ExTb.ProcDir.str.extract(r'/([\\d_]+).results')\n",
    "ExTb = pd.concat([tmp, ExTb],1)\n",
    "## This makes it harder to prepend columns:\n",
    "# ExTb[['Su','Se','Ex']] = ExTb.ProcDir.str.extract(r'^Su(\\d+)/Se(\\d+)/Ex(\\d+)/').astype(int);\n",
    "# ExTb['ExId'] = ExTb.ProcDir.str.extract(r'/([\\d_]+).results')\n",
    "# ExTb = pd.concat([ExTb.iloc[-4:],ExTb.iloc[0:-4]]) # sth like this should work\n",
    "\n",
    "ExTb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOWTO: Move an item in a list:\n",
    "hmoveitem = lambda L,A,B,n=0: L.insert( L.index(B)+n, L.pop( L.index(A)))\n",
    "# HOWTO: Move a column in a Pandas table:\n",
    "hmovecol = lambda df,A,B,n=0: df.insert( list(df).index(B)+n, A, df.pop(A))\n",
    "hmovecol = lambda df,A,B,n=0: df.insert( df.columns.get_loc(B)+n, A, df.pop(A))\n",
    "\n",
    "ExTb.sort_values( ['Su','NiiSz'], inplace=True) # sort by Su and size!\n",
    "ExTb['ExRank'] = ExTb.groupby('Su')['NiiSz'].rank(method='first').astype(int)\n",
    "hmovecol(ExTb,'ExRank','Ex',1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.ExRank.to_numpy()\n",
    "# Want neg indices?:\n",
    "ExTb.groupby('Su')['ExRank'].apply( lambda x: x-x.max()-1).to_numpy()\n",
    "# ...or quantiles?\n",
    "ExTb.groupby('Su')['ExRank'].apply( lambda x: x/x.max()).to_numpy()\n",
    "#< The middle experiment\n",
    "#< ExTb.groupby('Su')['ExRank'].median().astype(int).to_numpy()\n",
    "#< ExTb.groupby('Su')['ExRank'].median().to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Su1Vx1MedianMetric.csv\n",
    "\n",
    "#### NOTES\n",
    "\n",
    "* pd.concat is the main function\n",
    "* pd.append is shorthand for concat rows\n",
    "* pd.merge is similar (equvalent?) to concat, but mimics SQL\n",
    "* pd.join is shorthand for merge with certain options.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tb = pd.read_csv('~/matlab/htools1/hpy/Km_D1Su1Vxn_SsesMetrics.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb = ExTb.join( Tb.groupby(['Su','Se','Ex']).median(), on=['Su','Se','Ex'])\n",
    "ExTb.drop(columns='Sc',inplace=True)\n",
    "# ExTb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# tmp = Tb.drop(['Sc','Clipped'],1).groupby(['Su','Se','Ex']).median().groupby(['Su']).rank(method='first').astype(int)\n",
    "tmp = Tb.drop(['Sc','Clipped'],1).groupby(['Su','Se','Ex']).median()\n",
    "#tmp['ValRank'] = tmp.groupby(['Su'])['WRVFz'].rank(method='first').astype(int)\n",
    "tmp['ValRank'] = tmp.groupby(['Su'])['Pcorr'].rank(method='first').astype(int)\n",
    "#ExTb.set_index(['Su','Se','Ex'], drop=False, verify_integrity=True)\n",
    "ExTb = pd.merge(ExTb,tmp,how='left',on=['Su','Se','Ex']) #.sort_values(['Su','Se','Ex'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.sort_values( ['Su','ValRank','NiiSz'], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************\n",
    "Obsolete given above."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tb = pd.read_csv('~/matlab/htools1/hpy/Km_D1Su1Vxn_SuMinMedianWRVFz.csv')\n",
    "\n",
    "# idx = pd.MultiIndex.from_frame(ExTb.iloc[:,:3])\n",
    "idx = pd.MultiIndex.from_frame(ExTb[['Su','Se','Ex']])\n",
    "# idx.get_loc( eval( Tb.WRVFz[1]) ) # ++\n",
    "# list(Tb.WRVFz.apply(eval))\n",
    "# HOWTO select multiple rows from a PD DataFrame with multiindex:\n",
    "idx = list(map(idx.get_loc, list(Tb.WRVFz.apply(eval)))) # +++\n",
    "# Pythetically, pd.MultiIndex.get_loc is used instead of .loc\n",
    "\n",
    "### mk col ValRank for exp with best x-validation results\n",
    "ExTb['ValRank'] = np.nan # +++\n",
    "ExTb.ValRank[idx] = 1 # +++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb[ ExTb.ValRank==1 ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eval( Tb.WRVFz[0])\n",
    "Tb.WRVFz.apply(eval)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### This kind of works, but tediously!\n",
    "idx = ExTb.iloc[:,:3].values\n",
    "idx = np.r_['-1,3,0',idx]\n",
    "tmp = np.array( Tb.WRVFz.apply(eval).tolist() )\n",
    "tmp = np.r_['-1,3',tmp.T]\n",
    "np.nonzero(np.all(idx == tmp,1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Very unexpected results, probably does Matlab style list slicing!\n",
    "tmp = list( zip(*list(Tb.WRVFz.apply(eval))))\n",
    "tmp\n",
    "idx.get_locs( tmp ) # ++ probably does Matlab style list slicing!\n",
    "#idx.get_locs( list(Tb.WRVFz.apply(eval)) ) # ++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mk ExTb.ValTag\n",
    "mk ExTb.ValTag = 'ValSu' if ExTb.Su in Par.ValSu or 'ValEx' if shortest scan for each subj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1st, mk new ValTag column:\n",
    "def ExTb_ValTag( ExTb, Par):\n",
    "    # HOWTO delete a Pandas column in place without errors:\n",
    "    ExTb.drop( columns='ValTag', errors='ignore', inplace=True) # remove old column\n",
    "    ExTb.insert( 3, 'ValTag', False) # +++\n",
    "\n",
    "    if Par.ValRank:\n",
    "        ExTb['ExRank'] = ExTb.ValRank # +++\n",
    "        print('+ Using ExRank = ValRank')\n",
    "    if Par.ValEx1:\n",
    "        try: tmp = list(Par.ValEx1)\n",
    "        except: tmp = [ Par.ValEx1 ]\n",
    "        assert not any( np.array(tmp) > ExTb['ExRank'].max() ), 'Oops! Par.ValEx1 out of bounds.' \n",
    "        tmp = ExTb.ExRank.isin( tmp ) #.to_numpy()?\n",
    "        assert tmp.any(), 'Oops! ValEx1 out of bounds. Stop here.'\n",
    "        ExTb.loc[ tmp, 'ValTag'] = 'ValEx'\n",
    "\n",
    "    if Par.ValSu:\n",
    "        # ExTb.ValTag[ ExTb.Su.apply(lambda x: x in Par.ValSu) ] = 'ValSu'\n",
    "        ExTb.ValTag[ ExTb.Su.isin( Par.ValSu) ] = 'ValSu' # .eq() .isin() .notnull() ~ &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% 1st, mk new ValTag column:\n",
    "# HOWTO delete a Pandas column in place without errors:\n",
    "ExTb.drop( columns='ValTag', errors='ignore', inplace=True) # remove old column\n",
    "ExTb.insert( 3, 'ValTag', False) # +++"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if Par.ValEx1:\n",
    "    ExTb.sort_values( ['Su','NiiSz'], inplace=True) # sort by Su and size!\n",
    "    tmp = ExTb.groupby('Su')['NiiSz'].idxmin().to_numpy() # shortest scan for each Su -> ValEx tag\n",
    "    tmp = tmp + Par.ValEx1-1\n",
    "    ExTb['ValTag'].iloc[tmp] = 'ValEx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Par.ValRank:\n",
    "    ExTb['ExRank'] = ExTb.ValRank # +++\n",
    "    print('+ Using ExRank = ValRank')\n",
    "if Par.ValEx1:\n",
    "    try: tmp = list(Par.ValEx1)\n",
    "    except: tmp = [ Par.ValEx1 ]\n",
    "    assert not any( np.array(tmp) > ExTb['ExRank'].max() ), 'Oops! Par.ValEx1 out of bounds.' \n",
    "    tmp = ExTb.ExRank.isin( tmp ) #.to_numpy()?\n",
    "    assert tmp.any(), 'Oops! ValEx1 out of bounds. Stop here.'\n",
    "    ExTb.loc[ tmp, 'ValTag'] = 'ValEx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Par.ValSu:\n",
    "    # ExTb.ValTag[ ExTb.Su.apply(lambda x: x in Par.ValSu) ] = 'ValSu'\n",
    "    ExTb.ValTag[ ExTb.Su.isin( Par.ValSu) ] = 'ValSu' # .eq() .isin() .notnull() ~ &"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ExTb['ValSu'] = ExTb['Su'].apply(lambda x: x in Par.ValSu)\n",
    "ExTb.insert(3,'ValSu',ExTb['Su'].apply(lambda x: x in Par.ValSu)) # +++\n",
    "#< ExTb.Tags[ ExTb.ValSu ].apply(lambda x: x.append('ValSu'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.insert(4,'ValEx',ExTb.ValSu)\n",
    "# tmp = ExTb.groupby('Su',as_index=True)['NiiSz'].idxmin() # shortest scan for each Su\n",
    "tmp = ExTb.groupby('Su')['NiiSz'].idxmin().tolist() # shortest scan for each Su\n",
    "ExTb['ValEx'].iloc[tmp] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExTb.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.insert(3,'ExSec',np.nan)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Reorder columns?!\n",
    "ExTb = ExTb[['Su','Se','Ex']].merge(ExTb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # TODO: Test for SB_TASK\n",
    "    warn('+++ TEST avoid out of memory!')\n",
    "    # ExTb = ExTb[0:ExTb.shape[0]//2]\n",
    "    ExTb = ExTb[0:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load all Gsig files into dict (Pars) and append to ExTb\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pars = [ {'ProcDir':n} for n in ExTb.ProcDir.tolist() ] # +++ Pars\n",
    "\n",
    "# Add vars from Gsig.mat, see h_DsRnn_Gsig_sba\n",
    "# ['DataDir', 'Fs', 'Gsig', 'GsigLegendre', 'MatFile', 'McPar', 'McParFile', 'McRef', 'NiiDims', 'NiiFile', 'NiiMaskFile', 'Phy', 'PhyFile', 'ProcDir', 'SbJob', 'SbTask', 'SuId', 'Svd']\n",
    "for par in Pars: # Pars = list of experiments (scans) [ExTb,Exs]\n",
    "    # print('+ Load: '+par['ProcDir']+Par.GsigFile)\n",
    "    par.update( hdf5.loadmat(par['ProcDir']+Par.GsigFile) ) # +++\n",
    "    par.update( ExId = re.sub( '/run-([\\d_]+)/', '\\1', par.ProcDir))\n",
    "    print('.',end='')\n",
    "print(len(Pars))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### DUMP: Add sleep scoring - to be included below in mkData and hXtv2Data\n",
    "for par in Pars: # Pars = list of experiments (scans) [ExTb,Exs]\n",
    "    # print('+ Load: '+par['ProcDir']+Par.GsigFile)\n",
    "    par['HypnoFile'] = '/data/mandelkowhc/Sleep/EEG/'+par.ExId+'_sleepscoringmr.txt'\n",
    "    par['VmrkFile'] = '/data/mandelkowhc/Sleep/EEG/'+par.ExId+'.vmrk'\n",
    "    par['Hypno'] = readSsc( par.HypnoFile, par.VmrkFile)\n",
    "    par['VmrkT0'] = readVmrkStart( par.VmrkFile )\n",
    "    print('.',end='')\n",
    "print(len(Pars))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb = ExTb.merge( pd.DataFrame(Pars) )\n",
    "# del Pars # ???"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### TEST\n",
    "ExTb.merge(tmp,on='ProcDir')\n",
    "\n",
    "ExTb.merge(tmp,how='right')\n",
    "# ExTb.merge(tmp,left_index=True,right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "The above strategy using pd.merge is complicated and unreliable.\n",
    "\n",
    "Better to convert to/from list of dicts (records)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pars = ExTb.to_dict('records')\n",
    "\n",
    "# Add vars from Gsig.mat, see h_DsRnn_Gsig_sba\n",
    "# ['DataDir', 'Fs', 'Gsig', 'GsigLegendre', 'MatFile', 'McPar', 'McParFile', 'McRef', 'NiiDims', 'NiiFile', 'NiiMaskFile', 'Phy', 'PhyFile', 'ProcDir', 'SbJob', 'SbTask', 'SuId', 'Svd']\n",
    "for par in Pars: # Pars = list of experiments (scans) [ExTb,Exs]\n",
    "    # print('+ Load: '+par['ProcDir']+Par.GsigFile)\n",
    "    tmp = par.copy()\n",
    "    par.update( hdf5.loadmat(par['ProcDir']+Par.GsigFile) ) # +++\n",
    "    # par.update( ExId = re.sub( '/run-([\\d_]+)/', '\\1', par.ProcDir))\n",
    "    # par.update( Hypno= np.loadtxt(par['ProcDir']+'../Hypno.txt'), HypnoFile='../Hypno.txt')\n",
    "    par.update(tmp)\n",
    "    print('.',end='')\n",
    "print(len(Pars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExTb = pd.DataFrame(Pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExTb['HypnoFile'] = '../Hypno.txt'\n",
    "ExTb['Hypno'] = ExTb.apply(lambda row: np.loadtxt(row.ProcDir+row.HypnoFile), 'columns')\n",
    "# TODO: use max_rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ExTb.columns )\n",
    "ExTb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['HypnoFile'] = '/data/mandelkowhc/Sleep/EEG/'+ExTb.ExId+'_sleepscoringmr.txt'\n",
    "ExTb['VmrkFile'] = '/data/mandelkowhc/Sleep/EEG/'+ExTb.ExId+'.vmrk'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readVmrkStart( Fname ):\n",
    "    with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "    S = re.findall(r'^Mk1=New.*,(\\d+)$',T,re.M)[0]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['VmrkT0'] = ExTb.VmrkFile.apply( lambda x: readVmrkStart(x) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readVmrkR128( Fname ):\n",
    "    '''Rean MR triggers from EEG marker file .vmrk'''\n",
    "    with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "    S = re.findall(r'^Mk\\d+=Response,R128,(\\d+)',T,re.M)\n",
    "    S = list( map(int,S)) # int samples at 5kHz, 0.2ms?!\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readSsc( HypnoFile, VmrkFile=None, Crop=None ):\n",
    "    '''\n",
    "    Returns pd.Series of length NTR with sleep score N+(0.1 if REM) and index= orig. sleepscore intervall number n*30s\n",
    "    '''\n",
    "    # Ssc = pd.read_csv( Fname, '\\t', usecols='Score', squeeze=True)\n",
    "    Ssc = pd.read_csv( HypnoFile, '\\t')\n",
    "    S = Ssc.Score + (Ssc.REM/10.0)\n",
    "    if VmrkFile is not None:\n",
    "        tmp = ( np.array(readVmrkR128(VmrkFile))/30/5000 ).astype(int) # =floor() to Ssc frames (30s)\n",
    "        S = S.reindex(index=tmp) # =interpolates\n",
    "    if Crop is not None:\n",
    "        S = S[ int(Crop): ]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ExTb['Hypno'] = ExTb.apply( lambda x: readSsc(x.HypnoFile, x.VmrkFile).values[:x.Gsig.size], 'columns')\n",
    "ExTb['Hypno'] = ExTb.apply( lambda x: readSsc(x.HypnoFile, x.VmrkFile).reset_index(drop=True).reindex(range(x.Gsig.size)).values, 'columns')\n",
    "# FIXIT: Hypno can be 1 longer than Gsig?!? EEG MR trigger with missing MRI volume data. Acquisition interrupted?!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print( ExTb.columns )\n",
    "ExTb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data in ExTb\n",
    " * merge channels and ...\n",
    " * divide into equal length sections\n",
    " \n",
    "#### SEE: [h_DsRnn_v6b1_sba.ipynb](./h_DsRnn_v6b1_sba.ipynb#mkPars)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Easier to do this in one function, see below...\n",
    "# ExTb['Data'] = ExTb[['McPar','Gsig']].apply(lambda x: np.c_[x.McPar,x.Gsig],1)\n",
    "Par.setdefault('Fs',20.0)\n",
    "Par.setdefault('TR',3.0)\n",
    "Par.setdefault('TRFS',int(Par['Fs']*Par['TR']))\n",
    "ExTb['Data'] = ExTb[['Phy','McPar','Gsig']].apply(lambda x: hXtv2Data( x.Phy, np.c_[x.McPar,x.Gsig],Par.TRFS, MaskVal= Par.MaskVal),'columns')\n",
    "ExTb.Data = ExTb.Data.apply(lambda x: x[Par['CropData']:-Par['CropData']],'columns')\n",
    "ExTb.Data = ExTb.Data.apply(lambda x: (np.place( x, x*0+np.abs(x[:,-1:])>9.0, Par.MaskVal),x)[1],'columns')\n",
    "N = int(Par.Tbatch//Par['TR']* Par.TRFS) # 10 minutes\n",
    "#ExTb.Data = ExTb.Data.apply(lambda x: x[:x.shape[0]//N*N,:].reshape(-1,N,x.shape[-1]),'columns') # +++ Data[ NofSections, SectionLen, NX+NY]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#< Par.setdefault('Fs',20.0)\n",
    "#< Par.setdefault('TR',3.0)\n",
    "#< Par.setdefault('TRFS',int(Par['Fs']*Par['TR']))\n",
    "def mkData(row):\n",
    "    McPar = row.McPar # *** motion parameters\n",
    "    McPar = np.r_[ 0*McPar[:1,:], np.diff( McPar, axis=0 )] # *** Diff\n",
    "    McPar = np.c_[ McPar, np.linalg.norm(McPar,2,-1,True)] # *** add McNorm\n",
    "    \n",
    "    # For convenience, let's just append other suff to McPar:\n",
    "    # print( ( McPar.shape, row.Hypno.shape))\n",
    "    if McPar.shape[0] != row.Hypno.size:\n",
    "        warn('Size mismatch. Check this.')\n",
    "        # row.Hypno.resize( McPar.shape[0], refcheck=False)\n",
    "        # row.Hypno = np.append(row.Hypno,0*McPar[row.Hypno.size:])[:McPar.shape[0]]\n",
    "    # elseif True:\n",
    "    # McPar = np.c_[ McPar, row.Hypno ] # *** add sleep scores\n",
    "    McPar = np.c_[ McPar, np.append(row.Hypno,0*McPar[row.Hypno.size:])[:McPar.shape[0]] ] # *** add sleep scores\n",
    "\n",
    "    McPar = np.c_[ McPar, 1+0*McPar[:,:1] ] # *** add Mask\n",
    "\n",
    "    if Par['Xnul'] is not None:\n",
    "        row.Phy[:,Par['Xnul']] = 0.0 # +++ null chans for dropout purposes\n",
    "\n",
    "    Data = hXtv2Data( row.Phy, np.c_[ McPar, row.Gsig], Par.TRFS, MaskVal= Par.MaskVal) # ++++\n",
    "    \n",
    "    xi = row.Phy.shape[-1]\n",
    "    xi = slice( xi, xi+McPar.shape[-1]-1 ) # Need Gsig[0,:] != 0 to capture in x below\n",
    "    ti = np.flatnonzero( Data[:,-2] ) # Mask indices\n",
    "    Data[:,xi] = scipy.interpolate.interp1d( ti, Data[ti,xi], axis=0, copy=False, \\\n",
    "        bounds_error=False, fill_value=0, assume_sorted=True)(np.arange(Data.shape[0]))\n",
    "    # NOTE: copy=False matters, when the interpolation obj is to be used after the fitting data changes.\n",
    "    assert Data[0,xi].sum()==0, 'Oops! Should be zero!?'\n",
    "    # if Data[0,xi].sum()!=0: Data[:,xi] = np.nan\n",
    "\n",
    "    if Par.CropData: Data = Data[ Par.CropData:-Par.CropData ] # ++++\n",
    "    \n",
    "    Data[np.abs(Data[:,-1])>9.0,-1] = Par.MaskVal # *** mask outliers\n",
    "    \n",
    "    N = int(Par.Tbatch//Par['TR']* Par.TRFS) # 10 minutes\n",
    "    Data = Data[:Data.shape[0]//N*N,:].reshape(-1,N,Data.shape[-1]) # +++ Data[ NofSections, SectionLen, NX+NY]\n",
    "    return Data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Par['Xrnd'] is None, 'Xrnd not implemented!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#< ExTb['Data'] = ExTb[[n for n in ('Phy','McPar','Gsig','Hypno') if n in ExTb.columns]].apply(mkData,'columns') # +++ mk Data\n",
    "ExTb['Data'] = ExTb.apply(mkData,'columns') # +++ mk Data (much simpler!)\n",
    "# ExTb['ExSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0]) )\n",
    "# Par['DataChan'] = ['Card','Resp','Mc0','Mc1','Mc2','Mc3','Mc4','Mc5','McNorm','Mask','Gsig']\n",
    "Par['DataChan'] = ['Card','Resp','Mc0','Mc1','Mc2','Mc3','Mc4','Mc5','McNorm','Hypno','Mask','Gsig']\n",
    "ExTb.attr = pd.Series(Par) # attach (Ex)Par to ExTb\n",
    "# ExTb.meth = mkData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add col. McDifMax\n",
    "ExTb['McDifMax'] = ExTb['Data'].apply(lambda x: x[...,8].max(-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Fig: McDifMax\n",
    "hplotstyles()\n",
    "plt.plot(np.concatenate( ExTb['McDifMax'].tolist()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add col. ValSec = numeric tag for each data section\n",
    "# HOWTO remove Pandas columns whether or not they exist:\n",
    "ExTb.drop(columns=['TEST','ValSec'],inplace=True,errors='ignore') # remove preexisting cols.\n",
    "\n",
    "# HOWTO insert a Pandas column after another by name:\n",
    "ExTb.insert( ExTb.columns.get_loc('ValTag')+1,'ValSec',None) # insert after ValTag\n",
    "\n",
    "ExTb['ValSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0])==x.shape[0]//2)\n",
    "\n",
    "# ValSec = ValSec | ValEx | ValSu\n",
    "# HOWTO do np.logical_or on BOOLEAN arrays:\n",
    "# IFF operands are BOOLEAN you can use \"bitwise\" operators &,|,^, instead of the clumsy np.logical_*\n",
    "# ExTb.ValSec = ExTb.apply(lambda x: x.ValSec | (x.ValTag != False ),'columns')\n",
    "ExTb.ValSec = ExTb.apply( lambda x: np.logical_or( x.ValSec, x.ValTag != False ),'columns')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Redo ValSec labels: 1=val.section, 2=(ValTag!=False, e.g. ValEx), 3=(ValTag==ValSu)\n",
    "ExTb['ValSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0])==x.shape[0]//2)\n",
    "ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec.astype(int), 2*(x.ValTag != False)), 'columns')\n",
    "ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec.astype(int), 3*(x.ValTag == 'ValSu')), 'columns')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Redo ValSec labels: 1=val.section, 2=(ValTag!=False, e.g. ValEx), 3=(ValTag==ValSu)\n",
    "ExTb['ValSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0])/x.shape[0])\n",
    "ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec, 2*(x.ValTag != False)), 'columns')\n",
    "ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec, 3*(x.ValTag == 'ValSu')), 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redo ValSec labels: 1=val.section, 2=(ValTag!=False, e.g. ValEx), 3=(ValTag==ValSu)\n",
    "def ExTb_ValSec(ExTb):\n",
    "    # HOWTO remove Pandas columns whether or not they exist:\n",
    "    ExTb.drop(columns=['TEST','ValSec'],inplace=True,errors='ignore') # remove preexisting cols.\n",
    "\n",
    "    # HOWTO insert a Pandas column after another by name:\n",
    "    ExTb.insert( ExTb.columns.get_loc('ValTag')+1,'ValSec',None) # insert after ValTag\n",
    "    \n",
    "    # OLD: ver.1\n",
    "    # ExTb['ValSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0])==x.shape[0]//2)\n",
    "    # ExTb.ValSec = ExTb.apply( lambda x: np.logical_or( x.ValSec, x.ValTag != False ),'columns')\n",
    "\n",
    "    ExTb['ValSec'] = ExTb.Data.apply(lambda x: np.arange(x.shape[0])/x.shape[0])\n",
    "    ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec, 2*(x.ValTag != False)), 'columns')\n",
    "    ExTb.ValSec = ExTb.apply( lambda x: np.fmax( x.ValSec, 3*(x.ValTag == 'ValSu')), 'columns')\n",
    "\n",
    "\n",
    "ExTb_ValSec( ExTb )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hrowfun = lambda Tb,fun: Tb.apply( fun, 'columns')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ExTb['ValSec'] = ExTb.apply( lambda x: x.Data.shape[0] * [ x.ValTag ], 'columns' ).sum()\n",
    "# tmp = ExTb.Data.apply(lambda x: np.arange(x.shape[0])==x.shape[0]//2)\n",
    "tmp = ExTb.Data.apply(lambda x: [ str(n) for n in np.arange(x.shape[0])==x.shape[0]//2])\n",
    "tmp[ ExTb.ValTag != False ] = \n",
    "tmp[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = ExTb.apply( lambda x: x.Data.shape[0] * [ x.ValTag ] if ( x.ValTag != False )\n",
    "                 else [ str(n) for n in np.arange(x.Data.shape[0])==x.Data.shape[0]//2], 'columns' ).sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.groupby('ValTag').size()\n",
    "tmp.count('True')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = ExTb.apply( lambda x: x.Data.shape[0] * [ x.ValTag ], 'columns' ).sum()\n",
    "# An array (of objects) supports better indexing than a list:\n",
    "tmp = np.array( tmp, dtype=object ) # default creates fixed-length strings\n",
    "idx = np.concatenate( ExTb.Data.apply(lambda x: np.arange(x.shape[0])==x.shape[0]//2)) # bool array"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx[ tmp != False ] = False\n",
    "# idx[ tmp == False ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp[ idx ] = 'ValMid'\n",
    "# np.nonzero( tmp == 'ValMid' )\n",
    "tmp\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#np.array( tmp ) == 'ValEx' # False gets converted to string!\n",
    "# list(map( bool, tmp)) # Tedious!!\n",
    "# [ bool(n)  for n in tmp ]\n",
    "# [ 'ValEx' == n for n in tmp ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand ExTb into SecTb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Expand rows\n",
    "SecTb = [ (x.Su,x.Se,x.Ex,n,x.Data[n]) for x in ExTb.itertuples(index=False) for n in range(x.Data.shape[0]) ]\n",
    "#SecTb = [ (x[:3],x.Data[n]) for x in ExTb[['Su','Se','Ex','Data']].itertuples(index=False) for n in range(x.Data.shape[0]) ]\n",
    "SecTb = pd.DataFrame(SecTb,columns=['Su','Se','Ex','Sec','Data']) # tuples -> DF\n",
    "SecTb = SecTb.set_index(['Su','Se','Ex','Sec'], drop=False, append=False, inplace=False, verify_integrity=True)\n",
    "# SecTb.insert(SecTb.shape[1]-1,'ValSu',SecTb.Su.apply(Par.ValSu.count))\n",
    "SecTb['ValSu'] = SecTb.Su.apply(Par.ValSu.count)\n",
    "SecTb['ValSec'] = np.concatenate( ExTb.ValSec.tolist())\n",
    "SecTb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand rows\n",
    "# SecTb = [ (x.Su,x.Se,x.Ex,n,x.Data[n],x.ValSu,x.ValEx,x.ValSec[n]) for x in ExTb.itertuples(index=False) for n in range(x.Data.shape[0]) ]\n",
    "# SecTb = pd.DataFrame(SecTb,columns=['Su','Se','Ex','Sec','Data','ValSu','ValEx','ValSec']) # tuples -> DF\n",
    "SecTb = [ (x.Su, x.Se, x.Ex,n, x.ValTag, x.ValSec[n], x.Data[n]) for x in ExTb.itertuples(index=False) for n in range(x.Data.shape[0]) ]\n",
    "SecTb = pd.DataFrame(SecTb,columns=['Su','Se','Ex','Sec','ValTag','ValSec','Data']) # tuples -> DF\n",
    "SecTb = SecTb.set_index(['Su','Se','Ex','Sec'], drop=False, append=False, inplace=False, verify_integrity=True)\n",
    "SecTb = pd.concat( [SecTb.iloc[:,4:],SecTb.iloc[:,:4]],'columns')\n",
    "# SecTb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify sections with motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SecTb.McDifMax = each sections max. motion derivative\n",
    "SecTb['McDifMax'] = SecTb['Data'].apply(lambda x: x[...,8].max(-1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Fig: SecTb.McDifMax - Which sections have motion > 1mm?\n",
    "hplotstyles()\n",
    "SecTb['McDifMax'].plot(use_index=False)\n",
    "hhline([1.0])[0].set_color('r') # HOWTO hhline with color\n",
    "SecTb.ValTag[ SecTb.McDifMax > 1.0 ] = 'McThr'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Fig: Sections in for each ValTag\n",
    "hplotstyles()\n",
    "# plt.plot(SecTb.ValSec.values)\n",
    "# plt.plot(SecTb.ValEx.values)\n",
    "# plt.plot(SecTb.ValSu.values)\n",
    "plt.plot( SecTb.ValTag.str.match('ValEx').values == True)\n",
    "plt.plot( SecTb.ValTag.str.match('ValSu').values == True)\n",
    "plt.plot( SecTb.ValTag.str.match('McThr').values == True, 'r')\n",
    "plt.legend(['ValEx','ValSu','McThr']);\n",
    "#<SecTb.ValTag.str.match('ValEx').values == True\n",
    "#SecTb.ValTag.values\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Fig: Sections in for each ValTag\n",
    "hplotstyles()\n",
    "# SecTb.ValTag.str.match('ValEx').astype(float).plot.bar()\n",
    "tmp = np.arange(SecTb.shape[0])\n",
    "plt.bar(tmp, SecTb.ValTag.str.match('ValEx').values*1.4, width=1.)\n",
    "plt.bar(tmp, SecTb.ValTag.str.match('ValSu').values*1.2, width=1.)\n",
    "plt.bar(tmp, SecTb.ValTag.str.match('McThr').values*1.0, width=1.)\n",
    "plt.legend(['ValEx','ValSu','McThr'], loc='upper right');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.McPar[:5].apply(lambda x: x.shape)\n",
    "#ExTb.McPar.apply(lambda x: x.shape)\n",
    "# ExTb.NiiSz\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hplotstyles()\n",
    "# hplotchan = lambda X: plt.plot( (X-np.amin(X,0))/np.ptp(X,0) + np.arange(X.shape[1]) + 0.5 )\n",
    "# hplotchan = lambda X,kw={}: ( plt.plot( hrescale(X,axis=0) + np.arange(X.shape[1]) - 0.5, **kw), plt.yticks(np.arange(X.shape[1])))[0]\n",
    "hplotchan = lambda X,*arg: ( plt.plot( hrescale(X,axis=0) + np.arange(X.shape[1]) - 0.5, *arg), plt.yticks(np.arange(X.shape[1])))[0]\n",
    "hplotch = lambda X,*arg: ( plt.plot( hrescale(X,axis=None) + np.arange(X.shape[1]) - 0.5, *arg), plt.yticks(np.arange(X.shape[1])))[0]\n",
    "# This should preserve the relative scaling.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "tmp = np.concatenate(ExTb.Data.tolist(),0)[...,0]\n",
    "hplotchan(tmp.T);\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = np.concatenate( ExTb.Data.tolist(),0)[...,8] # [Sec,t,McDifSsq]\n",
    "tmp.shape\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Fig: McDifMax and McDifStd identify the same sections as outliers.\n",
    "# HOWTO have twin axes (twinx) with one common legend:\n",
    "hplotstyles()\n",
    "fig,ax = plt.subplots(1,1,figsize=(18,4))\n",
    "#plt.plot(tmp[20:30,:].T);\n",
    "h = plt.plot( tmp.max(1),label='McDifMAX')\n",
    "plt.twinx()\n",
    "# HOWTO collect handles for multiple line plots - just add:\n",
    "h += plt.plot(tmp.std(1),'r',label='McDifSTD')\n",
    "#hplotchan(tmp[20:40,:].T);\n",
    "hhline([tmp.std(1).mean()])\n",
    "hhline([tmp.std(1).mean()+tmp.std(1).std()*3]);\n",
    "np.sum(tmp.max(1)>1)\n",
    "#< plt.legend(['McDifMAX','McDifSTD'])\n",
    "plt.legend(h,['McDifMAX','McDifSTD']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************************************************\n",
    "## Compare section signals and spectra\n",
    "Might employ some sort of (spectral) clustering to identify poor-quality Physio signals?!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "hplotstyles()\n",
    "tmp = np.concatenate(ExTb.Data.tolist(),0)\n",
    "tmp = tmp[:,:,8]\n",
    "tmp.shape\n",
    "\n",
    "plt.plot(tmp[:,:].T);\n",
    "#plt.xlim(0,200)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp.shape\n",
    "# hdf5.savemat('tmp.mat',{'Sec':tmp})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = np.concatenate(ExTb.Data.tolist(),0)[...,0]\n",
    "tmp = hscalez(tmp,-1)\n",
    "tmp.shape\n",
    "\n",
    "NFFT = 2**10\n",
    "F = np.arange(NFFT)*Par.Fs/NFFT\n",
    "spc = np.fft.fft(tmp,NFFT,-1)\n",
    "(F,spc) = scsi.welch(tmp,20.0, nperseg=NFFT, nfft=NFFT, noverlap=None)\n",
    "spc.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(F[:NFFT//2],np.abs(spc[::80,:NFFT//2].T));\n",
    "# plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n = 0\n",
    "# n = np.flatnonzero(F>0.5)[0]\n",
    "x = F[np.abs(spc[:,n:2**10]).argmax(1)+n]\n",
    "plt.plot(x)\n",
    "hhline([0.5])\n",
    "(x<0.5).sum()\n",
    "plt.ylabel('Fmax > %g [Hz]'%F[n]);\n",
    "plt.xlabel('section [#]');\n",
    "plt.title('Cardiac peaks expected above 0.5Hz')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.plot(F[:2**10],np.abs(spc[x<0.5,:2**10].T));\n",
    "plt.title('POx spectra of sections with valid cardiac peaks')\n",
    "plt.xlim(0,4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(np.abs(spc[x<0.5,:2**10]))\n",
    "# plt.imshow(np.log10(np.abs(spc[x<0.5,:2**10])))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# %matplotlib auto\n",
    "%matplotlib inline\n",
    "hplotstyles()\n",
    "hplotchan(tmp[x>0.5,:].T);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************\n",
    "## Save ExTb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Save ExTb and (Proc)Par to .h5\n",
    "ExTb.to_hdf(Par.DataDir+'Proc/ExTb.h5','ExTb',format='table')\n",
    "pd.Series(Par).to_hdf(Par.DataDir+'Proc/ExTb.h5','ProcPar',format='table')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Save ExTb and (Proc)Par to .h5\n",
    "ExTb.to_hdf(Par.DataDir+'Proc/ExTb.h5','ExTb',format='table')\n",
    "pd.Series(Par).to_hdf(Par.DataDir+'Proc/ExTb.h5','ProcPar',format='table')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ExTb.ValTag[ExTb.ValTag==False] = ''\n",
    "ExTb.ValSec[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "SecTb.reset_index(drop=True).to_feather('SecTb.feather')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "SecTb.ValTag = SecTb.ValTag.astype(str)\n",
    "SecTb.reset_index(drop=True).to_parquet('SecTb.parquet')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SecTb.drop('Data',1).to_parquet('tmp.parquet')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SecTb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************************************\n",
    "## Intermezzo: Use Xarray?\n",
    "Remember, an xr.DataArray consists of an MD np.array [.values], a list of dimension names [.dims] (str), and optionally a dict of index vectors [.coords] corresponding to the [.dims].\n",
    "\n",
    "Dims+coords can also be passed as a list of tuples [('x',np.arange(10)),...] or PD DF or index objects."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xarray as xr\n",
    "Data = np.concatenate(ExTb.Data.tolist(),0)\n",
    "# Data = xr.DataArray(Data,dims=['Gsec','t','Chan'],coords={'Chan':list('123456789')})\n",
    "Data = xr.DataArray( Data, dims=['Exs','t','Ch'], coords={'Exs': SecTb.index, 'Ch':ExTb.attr.DataChan})\n",
    "# Data = xr.DataArray(Data,[('Exs',SecTb.index)]) # Def. dims+coords as list of tuples?!\n",
    "# Data.rename(['Gsec','t','Chan']) # rename dims?!\n",
    "# Data.set_index(ExTb[['Su','Se','Ex']]) # set dims+coords via PD.DF columns\n",
    "# Data.set_index(SecTb.index) # set dims+coords via PD.DF.index\n",
    "#Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xarray as xr\n",
    "#Data = np.concatenate(ExTb.Data.tolist(),0)\n",
    "# Data = xr.DataArray(Data,dims=['Gsec','t','Chan'],coords={'Chan':list('123456789')})\n",
    "# Data = xr.DataArray(SecTb.Data)\n",
    "# Data = xr.DataArray(np.stack( SecTb.Data.tolist(), 0), dims=['Exs','t','Ch'], coords={'Ch':ExTb.attr.DataChan})\n",
    "# HOWTO add a non-dimension coordinate to an Xarray.DataArray:\n",
    "# Data = Data.assign_coords({'Su':('Exs',SecTb.Su.tolist())})\n",
    "\n",
    "Data = xr.DataArray(np.stack( SecTb.Data.tolist(), 0), dims=['Exs','t','Ch'], coords={'Exs': SecTb.index, 'Ch':ExTb.attr.DataChan})\n",
    "# Data.rename(['Gsec','t','Chan'])\n",
    "# Data.set_index(ExTb[['Su','Se','Ex']])\n",
    "Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data[Data.Su==65,...]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SecTb.Su.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#SecTb.Data[0].shape\n",
    "Data = xr.DataArray(SecTb)\n",
    "Data.coords\n",
    "#Data = xr.DataArray( np.concatenate(SecTb.Data.tolist(),0), coords=Data.coords[:1] )\n",
    "#Data = xr.DataArray( np.stack(SecTb.Data.tolist(),0), coords=SecTb[['Su','Se','Ex','Sec']])\n",
    "Data = xr.DataArray( np.stack(SecTb.Data.tolist(),0), coords=[SecTb.index[0],SecTb.index[1]])\n",
    "Data.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***************************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb.groupby('Su').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************************************\n",
    "# EEG Sleep Scores\n",
    "Just divide the entire EEG duration into 30s intervals and assign them to the sleep scores.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['HypnoFile'] = '/data/mandelkowhc/Sleep/EEG/'+ExTb.ExId+'_sleepscoringmr.txt'\n",
    "ExTb['VmrkFile'] = '/data/mandelkowhc/Sleep/EEG/'+ExTb.ExId+'.vmrk'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readVmrkStart( Fname ):\n",
    "    with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "    S = re.findall(r'^Mk1=New.*,(\\d+)$',T,re.M)[0]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['VmrkT0'] = ExTb.VmrkFile.apply( lambda x: readVmrkStart(x) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readVmrkR128( Fname ):\n",
    "    '''Rean MR triggers from EEG marker file .vmrk'''\n",
    "    with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "    S = re.findall(r'^Mk\\d+=Response,R128,(\\d+)',T,re.M)\n",
    "    S = list( map(int,S)) # int samples at 5kHz, 0.2ms?!\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def readSsc( HypnoFile, VmrkFile=None, Crop=None ):\n",
    "    '''\n",
    "    Returns pd.Series of length NTR with sleep score N+(0.1 if REM) and index= orig. sleepscore intervall number n*30s\n",
    "    '''\n",
    "    # Ssc = pd.read_csv( Fname, '\\t', usecols='Score', squeeze=True)\n",
    "    Ssc = pd.read_csv( HypnoFile, '\\t')\n",
    "    S = Ssc.Score + (Ssc.REM/10.0)\n",
    "    if VmrkFile is not None:\n",
    "        tmp = ( np.array(readVmrkR128(VmrkFile))/30/5000 ).astype(int) # =floor() to Ssc frames (30s)\n",
    "        S = S.reindex(index=tmp) # =interpolates\n",
    "    if Crop is not None:\n",
    "        S = S[ int(Crop): ]\n",
    "    return S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['Hypno'] = ExTb.apply( lambda x: readSsc(x.HypnoFile, x.VmrkFile, 3*60/3).values, 'columns')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**************************************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ExTb['VmrkT0'] = ExTb.VmrkFile.apply( lambda x: readVmrkStart(x) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ssc = pd.read_csv( ExTb.HypnoFile[55],'\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ssc['Tmr'] = np.arange(Ssc.shape[0])*30*5000 - ExTb.VmrkTR0[55]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = (np.array(readVmrkR128(ExTb.VmrkFile[55]))/30/5000 ).astype(int)\n",
    "# Ssc.Score[ tmp ]\n",
    "Ssc.Score.reindex(index=tmp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ssc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "Ssc.Score.plot()\n",
    "Ssc.Score[tmp].plot()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HOWTO load text file as single string:\n",
    "# F = open(Fname, mode='r'); T = F.read(); F.close(); # T is one string.\n",
    "with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "T = re.search('(?:\\n%.*)+','\\n'+T) # = match obj!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "***********************************************"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import mne\n",
    "mne.io.read_raw_brainvision('20160518_0022.vmrk')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from configparser import ConfigParser\n",
    "cfg = ConfigParser()\n",
    "cfg.read('20160518_0022.vmrk')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fname = '20160518_0022.vmrk'\n",
    "with open(Fname, mode='r') as F: T = F.read() # T is one string.\n",
    "T = re.sub(r'^.*\\n\\[','[','\\n'+T,1,re.DOTALL)\n",
    "cfg.read_string(T)\n",
    "\n",
    "cfg.sections()\n",
    "list(cfg['Marker Infos'])\n",
    "cfg['Marker Infos']['mk1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
